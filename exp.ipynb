{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scalr Case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import argparse\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "import json\n",
    "import pathlib\n",
    "from torchinfo import summary\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "from PIL import Image\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='PyTorch Graphical VINF')\n",
    "\n",
    "parser.add_argument('-d', '--dataset', type=str, default='mnist', choices=['mnist'],\n",
    "                    metavar='DATASET',\n",
    "                    help='Dataset choice.')\n",
    "\n",
    "parser.add_argument('-nc', '--no_cuda', action='store_true', default=False,\n",
    "                    help='disables CUDA training')\n",
    "\n",
    "parser.add_argument('--manual_seed', type=int, help='manual seed, if not given resorts to random seed.')\n",
    "\n",
    "parser.add_argument('-li', '--log_interval', type=int, default=10, metavar='LOG_INTERVAL',\n",
    "                    help='how many batches to wait before logging training status')\n",
    "\n",
    "parser.add_argument('-od', '--out_dir', type=str, default='logs/', metavar='OUT_DIR',\n",
    "                    help='output directory for model snapshots etc.')\n",
    "\n",
    "fp = parser.add_mutually_exclusive_group(required=False)\n",
    "fp.add_argument('-te', '--testing', action='store_true', dest='testing',\n",
    "                help='evaluate on test set after training')\n",
    "fp.add_argument('-va', '--validation', action='store_false', dest='testing',\n",
    "                help='only evaluate on validation set')\n",
    "parser.set_defaults(testing=True)\n",
    "\n",
    "# optimization settings\n",
    "parser.add_argument('-e', '--epochs', type=int, default= 30, metavar='EPOCHS',\n",
    "                    help='number of epochs to train (default: 30)')\n",
    "parser.add_argument('-es', '--early_stopping_epochs', type=int, default=15, metavar='EARLY_STOPPING',\n",
    "                    help='number of early stopping epochs')\n",
    "\n",
    "parser.add_argument('-bs', '--batch_size', type=int, default=2000, metavar='BATCH_SIZE',\n",
    "                    help='input batch size for training (default: 100)')\n",
    "parser.add_argument('-lr', '--learning_rate', type=float, default=0.00001, metavar='LEARNING_RATE',\n",
    "                    help='learning rate')\n",
    "\n",
    "parser.add_argument('-a', '--anneal', type=str, default=\"std\", choices= [\"std\", \"off\", \"kl\"], help=\"beta annealing scheme\")\n",
    "parser.add_argument('--max_beta', type=float, default=1., metavar='MB',\n",
    "                    help='max beta for warm-up')\n",
    "parser.add_argument('--min_beta', type=float, default=0.0, metavar='MB',\n",
    "                    help='min beta for warm-up')\n",
    "parser.add_argument('-f', '--flow', type=str, default='planar', choices=['planar', 'NICE', 'NICE_MLP', 'real' ])\n",
    "parser.add_argument('-nf', '--num_flows', type=int, default=5,\n",
    "                    metavar='NUM_FLOWS', help='Number of flow layers, ignored in absence of flows')\n",
    "\n",
    "parser.add_argument('--z_size', type=int, default=1, metavar='ZSIZE',\n",
    "                    help='how many stochastic hidden units')\n",
    "\n",
    "\n",
    "parser.add_argument('-vp', '--vampprior', type=bool, default=True, metavar='VAMPPRIOR',\n",
    "                    help='choose whether to use VampPrior')\n",
    "\n",
    "parser.add_argument('--num_pseudos', type=int, default=100, metavar='NUM_PSEUDOS',\n",
    "                    help='number of pseudoinputs used for VampPrior')\n",
    "\n",
    "parser.add_argument('--data_as_pseudo', type=bool, default=True, metavar='data_as_pseudo',\n",
    "                    help='use random training data as pseudoinputs')\n",
    "\n",
    "# gpu/cpu\n",
    "parser.add_argument('--gpu_num', type=int, default=0, metavar='GPU', help='choose GPU to run on.')\n",
    "\n",
    "args = parser.parse_args(args=[])\n",
    "args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "\n",
    "if args.manual_seed is None:\n",
    "    args.manual_seed = 42\n",
    "random.seed(args.manual_seed)\n",
    "torch.manual_seed(args.manual_seed)\n",
    "np.random.seed(args.manual_seed)\n",
    "\n",
    "if args.cuda:\n",
    "    torch.cuda.set_device(args.gpu_num)\n",
    "args.data_as_pseudo = False\n",
    "\n",
    "args.batch_size = 200\n",
    "args.num_flows = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate the 'true' posterior and causal dependency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "total_num = 100000\n",
    "loc_LS, scale_LS = 0.5, 0.004\n",
    "pLS = np.random.logistic(loc_LS, scale_LS, total_num)\n",
    "\n",
    "loc_LF, scale_LF = 0.1, 0.003\n",
    "pLF = np.random.logistic(loc_LF, scale_LF, total_num)\n",
    "\n",
    "loc_BD, scale_BD = 0.8, 0.005\n",
    "pBD = np.random.logistic(loc_BD, scale_BD, total_num)\n",
    "\n",
    "count_LS, bins_LS, ignored_LS = plt.hist(pLS, bins=500)\n",
    "count_LF, bins_LF, ignored_LF = plt.hist(pLF, bins=500)\n",
    "count_BD, bins_BD, ignored_BD = plt.hist(pBD, bins=500)\n",
    "\n",
    "PLS = torch.from_numpy(pLS).float()\n",
    "\n",
    "PLF = torch.from_numpy(pLF).float()\n",
    "\n",
    "PBD = torch.from_numpy(pBD).float()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.8539), tensor(0.1344), tensor(0.5470))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PBD.max(),PLF.max(),PLS.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "PLS = PLS.view(-1,args.batch_size)\n",
    "PLF = PLF.view(-1,args.batch_size)\n",
    "PBD = PBD.view(-1,args.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_0LS = torch.tensor(0.2956)\n",
    "w_0LF = torch.tensor(0.6414)\n",
    "w_0BD = torch.tensor(0.2184)\n",
    "w_eLS = torch.tensor(0.0186)\n",
    "w_eLF = torch.tensor(0.9015)\n",
    "w_eBD = torch.tensor(0.1920)\n",
    "w_ex  = torch.tensor(0.0267)\n",
    "w_0x  = torch.tensor(0.5337)\n",
    "var_x = torch.tensor(0.1333)\n",
    "w_LSx = torch.tensor(0.2258)\n",
    "w_LFx = torch.tensor(0.6058)\n",
    "w_BDx = torch.tensor(0.3021)\n",
    "\n",
    "w_aBD  = torch.tensor(0.7886)\n",
    "w_aLS  = torch.tensor(0.4105)\n",
    "w_aLF  = torch.tensor(0.2108)\n",
    "\n",
    "w_gBD = torch.tensor(0.8761) \n",
    "w_LSBD = torch.tensor(0.9303)\n",
    "w_LFBD = torch.tensor(0.1363)\n",
    " \n",
    "w_LSBD = torch.tensor(0.2513)\n",
    "w_LFBD = torch.tensor(0.3612)\n",
    "\n",
    "m = nn.Sigmoid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnum = np.floor(total_num/args.batch_size)\n",
    "rand_idx = np.array(random.sample(range(int(bnum)*args.batch_size),int(bnum)*args.batch_size))\n",
    "IND = torch.from_numpy(rand_idx[0:int(bnum*args.batch_size)])\n",
    "IND = IND.view(-1, args.batch_size)\n",
    "IND = IND.numpy()\n",
    "\n",
    "aLS = np.array(PLS).reshape([total_num])\n",
    "aLS = aLS[rand_idx[0:int(bnum*args.batch_size)]]\n",
    "aLS = torch.from_numpy(np.array(aLS)).float()\n",
    "aLS = aLS.view(-1,args.batch_size)\n",
    "\n",
    "aLF = np.array(PLF).reshape([total_num])\n",
    "aLF = aLF[rand_idx[0:int(bnum*args.batch_size)]]\n",
    "aLF = torch.from_numpy(np.array(aLF)).float()\n",
    "aLF = aLF.view(-1,args.batch_size)\n",
    "\n",
    "aBD = np.array(PBD).reshape([total_num])\n",
    "aBD = aBD[rand_idx[0:int(bnum*args.batch_size)]]\n",
    "aBD = torch.from_numpy(np.array(aBD)).float()\n",
    "aBD = aBD.view(-1,args.batch_size)\n",
    "\n",
    "\n",
    "LS = aLS.detach()\n",
    "LF = aLF.detach()\n",
    "BD = aBD.detach()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, aLS, aLF,aBD):\n",
    "        self.aLS, self.aLF, self.aBD =  aLS, aLF, aBD\n",
    "    def __getitem__(self, idx):\n",
    "        return (self.aLS[idx], self.aLF[idx], self.aBD[idx])\n",
    "    def __len__(self):\n",
    "        return self.aLS.size(0)\n",
    "\n",
    "dataset = MyDataset(LS, LF, BD)\n",
    "MyDataLoader = DataLoader(dataset=dataset,shuffle=True)\n",
    "\n",
    "PLS1_1 = PLS.view(-1,1).detach()\n",
    "PLF1_1 = PLF.view(-1,1).detach()\n",
    "PBD1_1 = PBD.reshape(-1,1).detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reparameterize(mu, var):\n",
    "    \"\"\"\n",
    "    Samples z from a multivariate Gaussian with diagonal covariance matrix using the\n",
    "     reparameterization trick.\n",
    "    \"\"\"\n",
    "    m = nn.Sigmoid()\n",
    "    std = var.sqrt()\n",
    "    eps = torch.randn_like(std)\n",
    "    z = eps * std + mu\n",
    "#     z_max = z.max()\n",
    "#     z_min = z.min()\n",
    "#     z = (z-z_min)/(z_max - z_min)\n",
    "    return m(z)\n",
    "    # q_d \\in [0,1]\n",
    "\n",
    "def f(y):\n",
    "    return torch.log(1-torch.exp(-y))\n",
    "\n",
    "def flow_k(q, u, w, b):\n",
    "\n",
    "    h = nn.Tanh()\n",
    "    m = nn.Sigmoid()\n",
    "    \"\"\"\n",
    "    Computes the following transformation:\n",
    "    z' = z + u h( w^T z + b)\n",
    "    Input shapes:\n",
    "    shape u = (batch_size, z_size, 1)\n",
    "    shape w = (batch_size, 1, z_size)\n",
    "    shape b = (batch_size, 1, 1)\n",
    "    shape z = (batch_size, z_size).\n",
    "    \"\"\"\n",
    "    # Equation (10)\n",
    "    q = q.unsqueeze(2)\n",
    "    prod = torch.bmm(w, q) + b\n",
    "    f_q = q + u * h(prod) # this is a 3d vector\n",
    "    f_q = f_q.squeeze(2) # this is a 2d vector\n",
    "    \n",
    "\n",
    "    # compute logdetJ\n",
    "    # Equation (11)\n",
    "    psi = w * (1 - h(prod) ** 2)  # w * h'(prod)\n",
    "    # Equation (12)\n",
    "    log_det_jacobian = torch.log(torch.abs(1 + torch.bmm(psi, u)))\n",
    "    log_det_jacobian = log_det_jacobian.squeeze(2).squeeze(1)\n",
    "\n",
    "    return f_q, log_det_jacobian\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for aLS, aLF, aBD in MyDataLoader: \n",
    "    mu = nn.Sequential(nn.Linear(args.batch_size, args.batch_size),\n",
    "                   nn.Hardtanh(min_val=-2, max_val=2))\n",
    "    var = nn.Sequential(nn.Linear(args.batch_size, args.batch_size),\n",
    "                        nn.Softplus(),\n",
    "                        nn.Hardtanh(min_val=1, max_val=5))\n",
    "    amor_u = nn.Sequential(nn.Linear(args.batch_size, args.num_flows*args.batch_size),\n",
    "                           nn.Hardtanh(min_val=-1, max_val=1))\n",
    "    amor_w = nn.Sequential(nn.Linear(args.batch_size, args.num_flows*args.batch_size),\n",
    "                           nn.Hardtanh(min_val=-1, max_val=1))\n",
    "    amor_b = nn.Sequential(nn.Linear(args.batch_size, args.num_flows),\n",
    "                           nn.Hardtanh(min_val=-1, max_val=1))\n",
    "\n",
    "\n",
    "    aLS = aLS.view(-1,args.batch_size)\n",
    "    aLF = aLF.view(-1,args.batch_size)\n",
    "    aBD = aBD.view(-1,args.batch_size)\n",
    "    \n",
    "    PLS_1 = w_aLS*aLS + w_0LS\n",
    "    PLF_1 = w_aLF*aLF + w_0LF\n",
    "    q_LS_mu = mu(PLS_1) \n",
    "    y_LS_var = var(PLS_1)\n",
    "    u_LS = amor_u(PLS_1).view(-1, args.num_flows, args.batch_size, 1)\n",
    "    w_LS = amor_w(PLS_1).view(-1, args.num_flows, 1, args.batch_size)\n",
    "    b_LS = amor_b(PLS_1).view(-1, args.num_flows, 1, 1)\n",
    "    q_LS_var = y_LS_var + w_eLS**2    \n",
    "    q_LS = reparameterize(q_LS_mu, q_LS_var)\n",
    "    q_LS = [q_LS]  \n",
    "    \n",
    "    q_LF_mu = mu(PLF_1) \n",
    "    y_LF_var = var(PLF_1)\n",
    "    u_LF = amor_u(PLF_1).view(-1, args.num_flows, args.batch_size, 1)\n",
    "    w_LF = amor_w(PLF_1).view(-1, args.num_flows, 1, args.batch_size)\n",
    "    b_LF = amor_b(PLF_1).view(-1, args.num_flows, 1, 1)\n",
    "    q_LF_var = y_LF_var + w_eLF**2\n",
    "    q_LF = reparameterize(q_LF_mu, q_LF_var)\n",
    "    q_LF = [q_LF]\n",
    "\n",
    "    log_det_j_LS = 0.\n",
    "    log_det_j_LF = 0.\n",
    "    log_det_j_BD = 0.\n",
    "\n",
    "\n",
    "    for k in range(args.num_flows):\n",
    "        q_LFk, log_det_jacobian_LF = flow_k(q_LF[k], u_LF[:, k, :, :], w_LF[:, k, :, :], b_LF[:, k, :, :])\n",
    "        q_LF.append(q_LFk)\n",
    "        log_det_j_LF = log_det_j_LF + log_det_jacobian_LF\n",
    "\n",
    "    q_LF_0 = q_LF[0]\n",
    "    q_LF_K = m(q_LF[-1])\n",
    "\n",
    "    for k in range(args.num_flows):\n",
    "        q_LSk, log_det_jacobian_LS = flow_k(q_LS[k], u_LS[:, k, :, :], w_LS[:, k, :, :], b_LS[:, k, :, :])\n",
    "        q_LS.append(q_LSk)\n",
    "        log_det_j_LS = log_det_j_LS + log_det_jacobian_LS \n",
    "    q_LS_0 = q_LS[0]\n",
    "    q_LS_K = m(q_LS[-1])\n",
    "\n",
    "\n",
    "\n",
    "    PBD_1 = w_LSBD*q_LS_K + w_LFBD*q_LF_K + w_aBD*aBD + w_0BD\n",
    "\n",
    "    q_BD_mu = mu(PBD_1) \n",
    "    y_BD_var = var(PBD_1)\n",
    "    u_BD = amor_u(PBD_1).view(-1, args.num_flows, args.batch_size, 1)\n",
    "    w_BD = amor_w(PBD_1).view(-1, args.num_flows, 1, args.batch_size)\n",
    "    b_BD = amor_b(PBD_1).view(-1, args.num_flows, 1, 1)\n",
    "\n",
    "\n",
    "    q_BD_var = y_BD_var + w_eBD**2\n",
    "    q_BD = reparameterize(q_BD_mu, q_BD_var)\n",
    "    q_BD = [q_BD]\n",
    "\n",
    "    for k in range(args.num_flows):\n",
    "        q_BDk, log_det_jacobian_BD = flow_k(q_BD[k], u_BD[:, k, :, :], w_BD[:, k, :, :], b_BD[:, k, :, :])\n",
    "        q_BD.append(q_BDk)\n",
    "        log_det_j_BD = log_det_j_BD + log_det_jacobian_BD\n",
    "\n",
    "    q_BD_0 = q_BD[0]\n",
    "    q_BD_K = m(q_BD[-1])\n",
    "    \n",
    "    PLS1_1[IND[i]] = q_LS_K.view(-1,1)\n",
    "    PLF1_1[IND[i]] = q_LF_K.view(-1,1)\n",
    "    PBD1_1[IND[i]] = q_BD_K.view(-1,1)\n",
    "\n",
    "    \n",
    "    i = i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# PLS_1 = w_aLS*PLS + w_0LS\n",
    "# PLF_1 = w_aLF*PLF + w_0LF\n",
    "\n",
    "\n",
    "# q_LS_mu = mu(PLS_1) \n",
    "# y_LS_var = var(PLS_1)\n",
    "# u_LS = amor_u(PLS_1).view(-1, args.num_flows, args.batch_size, 1)\n",
    "# w_LS = amor_w(PLS_1).view(-1, args.num_flows, 1, args.batch_size)\n",
    "# b_LS = amor_b(PLS_1).view(-1, args.num_flows, 1, 1)\n",
    "# q_LS_var = y_LS_var + w_eLS**2    \n",
    "# q_LS = reparameterize(q_LS_mu, q_LS_var)\n",
    "# q_LS = [q_LS]  \n",
    "\n",
    "\n",
    "# q_LF_mu = mu(PLF_1) \n",
    "# y_LF_var = var(PLF_1)\n",
    "# u_LF = amor_u(PLF_1).view(-1, args.num_flows, args.batch_size, 1)\n",
    "# w_LF = amor_w(PLF_1).view(-1, args.num_flows, 1, args.batch_size)\n",
    "# b_LF = amor_b(PLF_1).view(-1, args.num_flows, 1, 1)\n",
    "# q_LF_var = y_LF_var + w_eLF**2\n",
    "# q_LF = reparameterize(q_LF_mu, q_LF_var)\n",
    "# q_LF = [q_LF]\n",
    "\n",
    "# log_det_j_LS = 0.\n",
    "# log_det_j_LF = 0.\n",
    "# log_det_j_BD = 0.\n",
    "\n",
    "\n",
    "# for k in range(args.num_flows):\n",
    "#     q_LFk, log_det_jacobian_LF = flow_k(q_LF[k], u_LF[:, k, :, :], w_LF[:, k, :, :], b_LF[:, k, :, :])\n",
    "#     q_LF.append(q_LFk)\n",
    "#     log_det_j_LF = log_det_j_LF + log_det_jacobian_LF\n",
    "    \n",
    "# q_LF_0 = q_LF[0]\n",
    "# q_LF_K = m(q_LF[-1])\n",
    "\n",
    "# for k in range(args.num_flows):\n",
    "#     q_LSk, log_det_jacobian_LS = flow_k(q_LS[k], u_LS[:, k, :, :], w_LS[:, k, :, :], b_LS[:, k, :, :])\n",
    "#     q_LS.append(q_LSk)\n",
    "#     log_det_j_LS = log_det_j_LS + log_det_jacobian_LS \n",
    "# q_LS_0 = q_LS[0]\n",
    "# q_LS_K = m(q_LS[-1])\n",
    "\n",
    "# PBD_1 = w_LSBD*q_LS_K + w_LFBD*q_LF_K + w_aBD*PBD + w_0BD\n",
    "\n",
    "# q_BD_mu = mu(PBD_1) \n",
    "# y_BD_var = var(PBD_1)\n",
    "# u_BD = amor_u(PBD_1).view(-1, args.num_flows, args.batch_size, 1)\n",
    "# w_BD = amor_w(PBD_1).view(-1, args.num_flows, 1, args.batch_size)\n",
    "# b_BD = amor_b(PBD_1).view(-1, args.num_flows, 1, 1)\n",
    "\n",
    "\n",
    "# q_BD_var = y_BD_var + w_eBD**2\n",
    "# q_BD = reparameterize(q_BD_mu, q_BD_var)\n",
    "# q_BD = [q_BD]\n",
    "\n",
    "# for k in range(args.num_flows):\n",
    "#     q_BDk, log_det_jacobian_BD = flow_k(q_BD[k], u_BD[:, k, :, :], w_BD[:, k, :, :], b_BD[:, k, :, :])\n",
    "# #     q_BDk = m(q_BDk)\n",
    "#     q_BD.append(q_BDk)\n",
    "#     log_det_j_BD = log_det_j_BD + log_det_jacobian_BD\n",
    "\n",
    "# q_BD_0 = q_BD[0]\n",
    "# q_BD_K = m(q_BD[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "PLS1_1 = PLS1_1.detach()\n",
    "PLF1_1 = PLF1_1.detach()\n",
    "PBD1_1 = PBD1_1.detach()\n",
    "h_sum = w_LSx*PLS1_1 + w_LFx*PLF1_1 + w_BDx*PBD1_1 + w_0x\n",
    "h_sum = h_sum.detach()\n",
    "OBS = np.random.lognormal(h_sum.numpy(),w_ex)\n",
    "OBS_max = np.max(OBS)\n",
    "OBS_min = np.min(OBS)\n",
    "OBS = (OBS - OBS_min)/(OBS_max - OBS_min)\n",
    "OBS[OBS == 0] = OBS[OBS > 0].min()\n",
    "OBS = torch.from_numpy(OBS).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Planar(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Planar, self).__init__()\n",
    "        self.h = nn.Tanh()\n",
    "\n",
    "    def forward(self, z, u, w, b):\n",
    "        \"\"\"\n",
    "        Computes the following transformation:\n",
    "        z' = z + u h( w^T z + b)\n",
    "        Input shapes:\n",
    "        shape u = (batch_size, 1, 1)\n",
    "        shape w = (batch_size, 1, 1)\n",
    "        shape b = (batch_size, 1, 1)\n",
    "        shape z = (batch_size, 1).\n",
    "        \"\"\"\n",
    "\n",
    "        # Equation (10)\n",
    "        z = z.unsqueeze(2)\n",
    "        prod = torch.bmm(w, z) + b\n",
    "        f_z = z + u * self.h(prod) # this is a 3d vector\n",
    "        f_z = f_z.squeeze(2) # this is a 2d vector\n",
    "#         print('w: ',w)\n",
    "#         print('prod: ', prod)\n",
    "#         print('f_z: ',f_z)\n",
    "\n",
    "        # compute logdetJ\n",
    "        # Equation (11)\n",
    "        psi = w * (1 - self.h(prod) ** 2)  # w * h'(prod)\n",
    "#         print('psi: ',psi)\n",
    "        # Equation (12)\n",
    "        log_det_jacobian = torch.log(torch.abs(1 + torch.bmm(psi, u)))\n",
    "        log_det_jacobian = log_det_jacobian.squeeze(2).squeeze(1)\n",
    "\n",
    "\n",
    "        return f_z, log_det_jacobian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 用Dataset封装数据集，仅做示范，实际可直接用TensorDataset封装\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, aLS, aLF,aBD,obs):\n",
    "        self.aLS, self.aLF, self.aBD, self.obs =  aLS, aLF, aBD, obs\n",
    "    #定义初始化变量\n",
    "    def __getitem__(self, idx):\n",
    "        return (self.aLS[idx], self.aLF[idx], self.aBD[idx], self.obs[idx])\n",
    "    #定义每次取出的对应数值\n",
    "    def __len__(self):\n",
    "        return self.aLS.size(0)\n",
    "    #定义tensor的总长度\n",
    "# 2. 用DataLoader定义数据批量迭代器\n",
    "\n",
    "def log_normal_dist(x, mean, logvar, dim):\n",
    "    log_norm = -0.5 * (logvar + (x - mean) * (x - mean) * logvar.exp().reciprocal()) \n",
    "\n",
    "    return torch.sum(log_norm, dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphicalVINF(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(GraphicalVINF, self).__init__()\n",
    "\n",
    "        # extract model settings from args\n",
    "        self.batch_size = args.batch_size\n",
    "        self.is_cuda = args.cuda\n",
    "        self.log_det_j_LS = 0.\n",
    "        self.log_det_j_LF = 0.\n",
    "        self.log_det_j_BD = 0.\n",
    "        self.num_pseudos = args.num_pseudos # for initialising pseudoinputs\n",
    "        \n",
    "        flowLS = Planar # For normalizing flow\n",
    "        flowLF = Planar\n",
    "        flowBD = Planar\n",
    "        self.num_flows = args.num_flows\n",
    "        self.m = nn.Sigmoid()\n",
    "        \n",
    "        # Normalizing flow layers\n",
    "        for k in range(self.num_flows):\n",
    "            flow_k_LS = flowLS()\n",
    "            self.add_module('flow_LS_' + str(k), flow_k_LS)\n",
    "        for k in range(self.num_flows):\n",
    "            flow_k_LF = flowLF()\n",
    "            self.add_module('flow_LF_' + str(k), flow_k_LF)\n",
    "        for k in range(self.num_flows):\n",
    "            flow_k_BD = flowBD()\n",
    "            self.add_module('flow_BD_' + str(k), flow_k_BD)\n",
    "        \n",
    "        \n",
    "        # Paramters in normalizing flows    \n",
    "        self.mu_LS = nn.Sequential(nn.Linear(self.batch_size, self.batch_size),\n",
    "                                   nn.Hardtanh(min_val=-1, max_val=1))\n",
    "        self.var_LS = nn.Sequential(nn.Linear(self.batch_size, self.batch_size),\n",
    "                                    nn.Softplus(),\n",
    "                                    nn.Hardtanh(min_val=1, max_val=5))\n",
    "        self.amor_u_LS = nn.Sequential(nn.Linear(self.batch_size, self.num_flows * self.batch_size),\n",
    "                                       nn.Hardtanh(min_val=-1, max_val=1))\n",
    "        self.amor_w_LS = nn.Sequential(nn.Linear(self.batch_size, self.num_flows * self.batch_size),\n",
    "                                       nn.Hardtanh(min_val=-1, max_val=1))\n",
    "        self.amor_b_LS = nn.Sequential(nn.Linear(self.batch_size, self.num_flows),\n",
    "                                       nn.Hardtanh(min_val=-1, max_val=1))\n",
    "\n",
    "        self.mu_LF = nn.Sequential(nn.Linear(self.batch_size, self.batch_size),\n",
    "                                   nn.Hardtanh(min_val=-1, max_val=1))\n",
    "        self.var_LF = nn.Sequential(nn.Linear(self.batch_size, self.batch_size),\n",
    "                                    nn.Softplus(),\n",
    "                                    nn.Hardtanh(min_val=1, max_val=5))\n",
    "        self.amor_u_LF = nn.Sequential(nn.Linear(self.batch_size, self.num_flows * self.batch_size),\n",
    "                                       nn.Hardtanh(min_val=-1, max_val=1))\n",
    "        self.amor_w_LF = nn.Sequential(nn.Linear(self.batch_size, self.num_flows * self.batch_size),\n",
    "                                       nn.Hardtanh(min_val=-1, max_val=1))\n",
    "        self.amor_b_LF = nn.Sequential(nn.Linear(self.batch_size, self.num_flows),\n",
    "                                       nn.Hardtanh(min_val=-1, max_val=1))\n",
    "\n",
    "        self.mu_BD = nn.Sequential(nn.Linear(self.batch_size, self.batch_size),\n",
    "                                   nn.Hardtanh(min_val=-1, max_val=1))\n",
    "        self.var_BD = nn.Sequential(nn.Linear(self.batch_size, self.batch_size),\n",
    "                                    nn.Softplus(),\n",
    "                                    nn.Hardtanh(min_val=1, max_val=5))\n",
    "        self.amor_u_BD = nn.Sequential(nn.Linear(self.batch_size, self.num_flows * self.batch_size),\n",
    "                                       nn.Hardtanh(min_val=-1, max_val=1))\n",
    "        self.amor_w_BD = nn.Sequential(nn.Linear(self.batch_size, self.num_flows * self.batch_size),\n",
    "                                       nn.Hardtanh(min_val=-1, max_val=1))\n",
    "        self.amor_b_BD = nn.Sequential(nn.Linear(self.batch_size, self.num_flows),\n",
    "                                       nn.Hardtanh(min_val=-1, max_val=1))\n",
    "\n",
    "        #nn.Softplus() = log(1+exp(x))\n",
    "        #Amortized flow parameters\n",
    "        # Parameters Setup\n",
    "        self.w_0LS = nn.Parameter(torch.rand(1, requires_grad=True, dtype=torch.float))\n",
    "        self.w_0LF = nn.Parameter(torch.rand(1, requires_grad=True, dtype=torch.float))\n",
    "        self.w_0BD = nn.Parameter(torch.rand(1, requires_grad=True, dtype=torch.float))\n",
    "        self.w_eLS = nn.Parameter(torch.rand(1, requires_grad=True, dtype=torch.float))\n",
    "        self.w_eLF = nn.Parameter(torch.rand(1, requires_grad=True, dtype=torch.float))\n",
    "        self.w_eBD = nn.Parameter(torch.rand(1, requires_grad=True, dtype=torch.float))\n",
    "        self.w_ex  = nn.Parameter(torch.rand(1, requires_grad=True, dtype=torch.float))\n",
    "        self.w_0x  = nn.Parameter(torch.rand(1, requires_grad=True, dtype=torch.float))\n",
    "        self.w_LSx = nn.Parameter(torch.rand(1, requires_grad=True, dtype=torch.float)) \n",
    "        self.w_LFx = nn.Parameter(torch.rand(1, requires_grad=True, dtype=torch.float)) \n",
    "        self.w_LSBD = nn.Parameter(torch.rand(1, requires_grad=True, dtype=torch.float)) \n",
    "        self.w_LFBD = nn.Parameter(torch.rand(1, requires_grad=True, dtype=torch.float)) \n",
    "        self.w_BDx = nn.Parameter(torch.rand(1, requires_grad=True, dtype=torch.float)) \n",
    "        self.w_aBD  = nn.Parameter(torch.rand(1, requires_grad=True, dtype=torch.float))\n",
    "        self.w_aLS  = nn.Parameter(torch.rand(1, requires_grad=True, dtype=torch.float))\n",
    "        self.w_aLF  = nn.Parameter(torch.rand(1, requires_grad=True, dtype=torch.float))\n",
    "        self.w_gBD = nn.Parameter(torch.rand(1, requires_grad=True, dtype=torch.float)) \n",
    "        self.t_gBD = nn.Parameter(torch.rand(1, requires_grad=True, dtype=torch.float))\n",
    "        self.t_aBD = nn.Parameter(torch.rand(1, requires_grad=True, dtype=torch.float))\n",
    "        self.t_aLS = nn.Parameter(torch.rand(1, requires_grad=True, dtype=torch.float))\n",
    "        self.t_aLF = nn.Parameter(torch.rand(1, requires_grad=True, dtype=torch.float))        \n",
    "\n",
    "        self.min_thres = 0.1\n",
    "        self.max_thres = 0.5\n",
    "        if args.cuda:\n",
    "            self.FloatTensor = torch.cuda.FloatTensor\n",
    "        else:\n",
    "            self.FloatTensor = torch.FloatTensor\n",
    "          \n",
    "    def reparameterize(self, mu, var):\n",
    "        \"\"\"\n",
    "        Samples z from a multivariate Gaussian with diagonal covariance matrix using the\n",
    "         reparameterization trick.\n",
    "        \"\"\"\n",
    "        std = var.sqrt()\n",
    "        eps = torch.randn_like(std)\n",
    "        z = eps * std + mu\n",
    "#         z_max = z.max()\n",
    "#         z_min = z.min()\n",
    "#         z = (z-z_min)/(z_max - z_min)\n",
    "        return self.m(z)\n",
    "\n",
    "    def f(self,y):\n",
    "        return torch.log(1-torch.exp(-y))\n",
    "        \n",
    "    def forward(self, a_LS, a_LF, a_BD, x):      \n",
    "        if self.is_cuda:\n",
    "            self.log_det_j_LS = torch.zeros([x.shape[0]]).cuda()\n",
    "            self.log_det_j_LF = torch.zeros([x.shape[0]]).cuda()\n",
    "            self.log_det_j_BD = torch.zeros([x.shape[0]]).cuda()\n",
    "        else:\n",
    "            self.log_det_j_LS = torch.zeros([x.shape[0]])\n",
    "            self.log_det_j_LF = torch.zeros([x.shape[0]])\n",
    "            self.log_det_j_BD = torch.zeros([x.shape[0]])\n",
    "\n",
    "        a_LS_1 = self.w_aLS*a_LS + self.w_0LS\n",
    "        a_LF_1 = self.w_aLF*a_LF + self.w_0LF\n",
    "        \n",
    "#         a_LS_1_max = a_LS_1.max()\n",
    "#         a_LS_1_min = a_LS_1.min()\n",
    "        a_LS_1 = self.m(a_LS_1)\n",
    "#     (a_LS_1 - a_LS_1_min)/(a_LS_1_max - a_LS_1_min)\n",
    "        \n",
    "#         a_LF_1_max = a_LF_1.max()\n",
    "#         a_LF_1_min = a_LF_1.min()\n",
    "        a_LF_1 = self.m(a_LF_1)\n",
    "#     (a_LF_1 - a_LF_1_min)/(a_LF_1_max - a_LF_1_min)\n",
    "        \n",
    "        \n",
    "        q_LS_mu = self.mu_LS(a_LS_1) \n",
    "        y_LS_var = self.var_LS(a_LS_1)\n",
    "        u_LS = self.amor_u_LS(a_LS_1).view(-1, self.num_flows, self.batch_size, 1)\n",
    "        w_LS = self.amor_w_LS(a_LS_1).view(-1, self.num_flows, 1, self.batch_size)\n",
    "        b_LS = self.amor_b_LS(a_LS_1).view(-1, self.num_flows, 1, 1)\n",
    "        q_LS_var = y_LS_var + self.w_eLS**2    \n",
    "        q_LS = self.reparameterize(q_LS_mu, q_LS_var)\n",
    "        q_LS = [q_LS]  \n",
    "\n",
    "        q_LF_mu = self.mu_LF(a_LS_1) \n",
    "        y_LF_var = self.var_LF(a_LS_1)\n",
    "        u_LF = self.amor_u_LF(a_LS_1).view(-1, self.num_flows, self.batch_size, 1)\n",
    "        w_LF = self.amor_w_LF(a_LS_1).view(-1, self.num_flows, 1, self.batch_size)\n",
    "        b_LF = self.amor_b_LF(a_LS_1).view(-1, self.num_flows, 1, 1)\n",
    "        q_LF_var = y_LF_var + self.w_eLF**2    \n",
    "        q_LF = self.reparameterize(q_LS_mu, q_LS_var)\n",
    "        q_LF = [q_LF]  \n",
    "\n",
    "        for k in range(self.num_flows):\n",
    "            flow_k_LS = getattr(self, 'flow_LS_' + str(k))\n",
    "            q_LSk, log_det_jacobian_LS = flow_k_LS(q_LS[k], u_LS[:, k, :, :], w_LS[:, k, :, :], b_LS[:, k, :, :])\n",
    "#             LS_min = q_LSk.min()\n",
    "#             LS_max = q_LSk.max()\n",
    "            q_LSk = self.m(q_LSk)\n",
    "#     (q_LSk - LS_min)/(LS_max - LS_min)\n",
    "            q_LS.append(q_LSk)\n",
    "            self.log_det_j_LS = self.log_det_j_LS + log_det_jacobian_LS  \n",
    "            \n",
    "        for k in range(self.num_flows):\n",
    "            flow_k_LF = getattr(self, 'flow_LF_' + str(k))\n",
    "            q_LFk, log_det_jacobian_LF = flow_k_LF(q_LF[k], u_LF[:, k, :, :], w_LF[:, k, :, :], b_LF[:, k, :, :])\n",
    "#             LF_min = q_LFk.min()\n",
    "#             LF_max = q_LFk.max()\n",
    "            q_LFk = self.m(q_LFk)\n",
    "#     (q_LFk - LF_min)/(LF_max - LF_min)\n",
    "            q_LF.append(q_LFk)\n",
    "            self.log_det_j_LF = self.log_det_j_LF + log_det_jacobian_LF\n",
    "\n",
    "        q_LS_0 = q_LS[0]\n",
    "        q_LF_0 = q_LF[0]\n",
    "        q_LS_K = q_LS[-1]\n",
    "        q_LF_K = q_LF[-1]\n",
    "        \n",
    "        g = torch.ones([self.batch_size,1])\n",
    "        g[(q_LF_K.view(self.batch_size,-1) <= self.min_thres) & (q_LS_K.view(self.batch_size,-1) <= self.min_thres)] = 0\n",
    "        g[(q_LF_K.view(self.batch_size,-1) >= self.max_thres) & (q_LS_K.view(self.batch_size,-1) >= self.max_thres)] = 0\n",
    "        g = g.view(a_LS.size(0),a_LS.size(1))\n",
    "\n",
    "        a_BD_1 = self.w_gBD*g + self.w_aBD*a_BD + self.w_0BD\n",
    "#         a_BD_1_max = a_BD_1.max()\n",
    "#         a_BD_1_min = a_BD_1.min()\n",
    "        a_BD_1 = self.m(a_BD_1)\n",
    "#     (a_BD_1 - a_BD_1_min)/(a_BD_1_max - a_BD_1_min)\n",
    "        \n",
    "        q_BD_mu = self.mu_BD(a_BD_1) \n",
    "        y_BD_var = self.var_BD(a_BD_1)\n",
    "        u_BD = self.amor_u_BD(a_BD_1).view(-1, self.num_flows, self.batch_size, 1)\n",
    "        w_BD = self.amor_w_BD(a_BD_1).view(-1, self.num_flows, 1, self.batch_size)\n",
    "        b_BD = self.amor_b_BD(a_BD_1).view(-1, self.num_flows, 1, 1)\n",
    "        q_BD_var = y_BD_var + self.w_eBD**2    \n",
    "        q_BD = self.reparameterize(q_BD_mu, q_BD_var)\n",
    "        q_BD = [q_BD]  \n",
    "\n",
    "\n",
    "        for k in range(self.num_flows):\n",
    "            flow_k_BD = getattr(self, 'flow_BD_' + str(k))\n",
    "            q_BDk, log_det_jacobian_BD = flow_k_BD(q_BD[k], u_BD[:, k, :, :], w_BD[:, k, :, :], b_BD[:, k, :, :])\n",
    "#             BD_min = q_BDk.min()\n",
    "#             BD_max = q_BDk.max()\n",
    "            q_BDk = self.m(q_BDk)\n",
    "#     (q_BDk - BD_min)/(BD_max - BD_min)\n",
    "            q_BD.append(q_BDk)\n",
    "            self.log_det_j_BD = self.log_det_j_BD + log_det_jacobian_BD \n",
    "        q_BD_0 = q_BD[0]\n",
    "        q_BD_K = q_BD[-1]\n",
    "        \n",
    "        \n",
    "\n",
    "#         E_log_p_z_BD = q_BD_K*(-torch.log(1+((torch.exp(-self.w_gBD)*p_g_1 + p_g_0)*torch.exp(self.w_eBD**2/2 - self.w_aBD*a_BD - self.w_0BD)))) + (1-q_BD_K)*(-torch.log(1+((torch.exp(self.w_gBD)*p_g_1 + p_g_0)*torch.exp(self.w_eBD**2/2 + self.w_aBD*a_BD + self.w_0BD))))\n",
    "#         E_log_p_z_LS = q_LS_K*(-torch.log(1+torch.exp(-self.w_0LS - self.w_aLS*a_LS + self.w_eLS**2/2))) + (1-q_LS_K)*(-torch.log(1+torch.exp(self.w_0LS + self.w_aLS*a_LS + self.w_eLS**2/2)))\n",
    "#         E_log_p_z_LF = q_LF_K*(-torch.log(1+torch.exp(-self.w_0LF - self.w_aLF*a_LF + self.w_eLF**2/2))) + (1-q_LF_K)*(-torch.log(1+torch.exp(self.w_0LF + self.w_aLF*a_LF + self.w_eLF**2/2)))\n",
    "\n",
    "        \n",
    "    \n",
    "#         BD_inner_1 = torch.exp(- self.w_LSBD - self.w_LFBD)*q_LS_K*q_LF_K + (1-q_LS_K)*(1-q_LF_K) + torch.exp(-self.w_LSBD)*q_LS_K*(1-q_LF_K)+ torch.exp(-self.w_LFBD)*q_LF_K*(1-q_LS_K)\n",
    "#         BD_inner_2 = torch.exp(self.w_LSBD + self.w_LFBD)*q_LS_K*q_LF_K + (1-q_LS_K)*(1-q_LF_K) + torch.exp(self.w_LSBD)*q_LS_K*(1-q_LF_K)+ torch.exp(self.w_LFBD)*q_LF_K*(1-q_LS_K)\n",
    "#         p_z_BD = (q_BD_K*(-torch.log(1+torch.exp(-(self.w_eBD**2)/2 - self.w_0BD - self.w_aBD*a_BD)*BD_inner_1))\n",
    "#                  +(1-q_BD_K)*(-torch.log(1+torch.exp((self.w_eBD**2)/2 + self.w_0BD + self.w_aBD*a_BD)*BD_inner_2)))\n",
    "#         p_z_LS = (q_LS_K*(-torch.log(1+torch.exp(-(self.w_eLS**2)/2 - self.w_0LS - self.w_aLS*a_LS)))\n",
    "#                 + (1-q_LS_K)*(-torch.log(1+torch.exp((self.w_eLS**2)/2 + self.w_0LS + self.w_aLS*a_LS))))\n",
    "#         p_z_LF = (q_LF_K*(-torch.log(1+torch.exp(-(self.w_eLF**2)/2 - self.w_0LF - self.w_aLF*a_LF)))\n",
    "#                 + (1-q_LF_K)*(-torch.log(1+torch.exp((self.w_eLF**2)/2 + self.w_0LF + self.w_aLF*a_LF))))\n",
    "        p_g_1 = q_LS_K*(1-q_LF_K) + q_LF_K*(1-q_LS_K)\n",
    "        p_g_0 = q_LS_K*q_LF_K + (1-q_LS_K)*(1-q_LF_K)    \n",
    "        E_log_p_z_BD = q_BD_K*(-torch.log(1+torch.exp(-self.w_0BD - self.w_aBD*a_BD - (self.w_eBD**2)/2)*(torch.exp(-self.w_gBD)*p_g_1 + p_g_0))) + (1-q_BD_K)*(-torch.log(1+torch.exp(-self.w_0BD - self.w_aBD*a_BD - (self.w_eBD**2)/2)*(torch.exp(self.w_gBD)*p_g_1+p_g_0)))\n",
    "        E_log_p_z_LS = q_LS_K*(-torch.log(1+torch.exp(-self.w_0LS - self.w_aLS*a_LS + (self.w_eLS**2)/2))) + (1-q_LS_K)*(-torch.log(1+torch.exp(self.w_0LS + self.w_aLS*a_LS + (self.w_eLS**2)/2)))\n",
    "        E_log_p_z_LF = q_LF_K*(-torch.log(1+torch.exp(-self.w_0LF - self.w_aLF*a_LF + (self.w_eLF**2)/2))) + (1-q_LF_K)*(-torch.log(1+torch.exp(self.w_0LF + self.w_aLF*a_LF + (self.w_eLF**2)/2)))\n",
    "        E_log_p_z = torch.mean(E_log_p_z_BD + E_log_p_z_LS + E_log_p_z_LF)        \n",
    "\n",
    "#         E_g = q_LS_K + q_LF_K - 2*q_LS_K*q_LF_K\n",
    "#         r_gBD = self.w_0BD + self.w_gBD/self.t_gBD\n",
    "#         E_log_p_z_BD =q_BD_K*(self.f(self.w_0BD) + self.t_gBD*E_g*(self.f(r_gBD) - self.f(self.w_0BD)) \n",
    "#                      + self.t_aBD*a_BD*(self.f(r_aBD) - self.f(self.w_0BD))) + (1-q_BD_K)*(-self.w_gBD*E_g - self.w_aBD*a_BD - self.w_0BD)                                                           \n",
    "#         r_aLS = self.w_0LS + self.w_aLS/self.t_aLS  \n",
    "#         E_log_p_z_LS = q_LS_K*(self.f(self.w_0LS) + self.t_aLS*a_LS*(self.f(r_aLS)-self.f(self.w_0LS))) + (1-q_LS_K)*(-self.w_aLS*a_LS - self.w_0LS)\n",
    "#         r_aLF = self.w_0LF + self.w_aLF/self.t_aLF                                                      \n",
    "#         E_log_p_z_LF = q_LF_K*(self.f(self.w_0LF) + self.t_aLF*a_LF*(self.f(r_aLF)-self.f(self.w_0LF))) + (1-q_LF_K)*(-self.w_aLF*a_LF - self.w_0LF)                                               \n",
    "#         E_log_p_z = torch.mean(E_log_p_z_BD + E_log_p_z_LS + E_log_p_z_LF)\n",
    "\n",
    "        E_log_p_xz = (- torch.log(x) - torch.log(torch.abs(torch.sqrt(self.w_ex))) \n",
    "                   - (torch.log(x)**2)/(2*(self.w_ex))\n",
    "                   + (torch.log(x)*(self.w_BDx*q_BD_K + self.w_LSx*q_LS_K + self.w_LFx*q_LF_K + self.w_0x))/(self.w_ex)\n",
    "                   - ((self.w_BDx**2)*q_BD_K + (self.w_LSx**2)*q_LS_K + (self.w_LFx**2)*q_LF_K + self.w_ex**2 + self.w_0x**2)/(2*(self.w_ex))\n",
    "                   - (self.w_BDx*self.w_LSx*q_BD_K*q_LS_K + self.w_BDx*self.w_LFx*q_BD_K*q_LF_K + self.w_LSx*self.w_LFx*q_LS_K*q_LF_K)/(self.w_ex)\n",
    "                   - (self.w_BDx*self.w_0x*q_BD_K + self.w_LSx*self.w_0x*q_LS_K + self.w_LFx*self.w_0x*q_LF_K)/(self.w_ex))\n",
    "#         E_log_p_xz = -torch.log(x) - torch.log(torch.abs(self.w_ex)) - ((torch.log(x))**2 + self.w_0x**2)/(2*self.w_ex**2) - ((self.w_BDx**2)*q_BD_K + (self.w_LSx**2)*q_LS_K + (self.w_LFx**2)*q_LF_K)/(2*self.w_ex**2) - (self.w_0x*(self.w_BDx*q_BD_K + self.w_LSx*q_LS_K + self.w_LFx*q_LF_K))/(self.w_ex**2) - (self.w_BDx*self.w_LSx*q_BD_K*q_LS_K + self.w_BDx*self.w_LFx*q_BD_K*q_LF_K + self.w_LFx*self.w_LSx*q_LF_K*q_LS_K)/(self.w_ex**2) + (torch.log(x)*(self.w_0x + self.w_BDx*q_BD_K + self.w_LSx*q_LS_K + self.w_LFx*q_LF_K))/(self.w_ex**2) \n",
    "        return q_LS_mu, q_LS_var, q_LS_0, q_LS_K, self.log_det_j_LS, q_LF_mu, q_LF_var, q_LF_0, q_LF_K, self.log_det_j_LF, q_BD_mu, q_BD_var, q_BD_0, q_BD_K, self.log_det_j_BD, E_log_p_xz, E_log_p_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphicalVINF(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(GraphicalVINF, self).__init__()\n",
    "\n",
    "        # extract model settings from args\n",
    "        self.batch_size = args.batch_size\n",
    "        self.is_cuda = args.cuda\n",
    "        self.log_det_j_LS = 0.\n",
    "        self.log_det_j_LF = 0.\n",
    "        self.log_det_j_BD = 0.\n",
    "        self.num_pseudos = args.num_pseudos # for initialising pseudoinputs\n",
    "        \n",
    "        flowLS = Planar # For normalizing flow\n",
    "        flowLF = Planar\n",
    "        flowBD = Planar\n",
    "        self.num_flows = args.num_flows\n",
    "        self.m = nn.Sigmoid()\n",
    "        \n",
    "        # Normalizing flow layers\n",
    "        for k in range(self.num_flows):\n",
    "            flow_k_LS = flowLS()\n",
    "            self.add_module('flow_LS_' + str(k), flow_k_LS)\n",
    "        for k in range(self.num_flows):\n",
    "            flow_k_LF = flowLF()\n",
    "            self.add_module('flow_LF_' + str(k), flow_k_LF)\n",
    "        for k in range(self.num_flows):\n",
    "            flow_k_BD = flowBD()\n",
    "            self.add_module('flow_BD_' + str(k), flow_k_BD)\n",
    "        \n",
    "        \n",
    "        # Paramters in normalizing flows\n",
    "        self.mu = nn.Sequential(nn.Linear(self.batch_size, self.batch_size),\n",
    "                                nn.Hardtanh(min_val=-2, max_val=2))\n",
    "        self.var = nn.Sequential(\n",
    "            nn.Linear(self.batch_size, self.batch_size),\n",
    "            nn.Softplus(),\n",
    "            nn.Hardtanh(min_val=1, max_val=5))\n",
    "        self.amor_u = nn.Sequential(nn.Linear(self.batch_size, self.num_flows * self.batch_size),\n",
    "                                    nn.Hardtanh(min_val=-1, max_val=1))\n",
    "        self.amor_w = nn.Sequential(nn.Linear(self.batch_size, self.num_flows * self.batch_size),\n",
    "                                    nn.Hardtanh(min_val=-1, max_val=1))\n",
    "        self.amor_b = nn.Sequential(nn.Linear(self.batch_size, self.num_flows),\n",
    "                                    nn.Hardtanh(min_val=-1, max_val=1))\n",
    "\n",
    "\n",
    "        \n",
    "        self.begin = nn.Sequential(\n",
    "            nn.Linear(self.batch_size,self.batch_size),\n",
    "            nn.Sigmoid())\n",
    "        #nn.Softplus() = log(1+exp(x))\n",
    "        #Amortized flow parameters\n",
    "\n",
    "\n",
    "        # Parameters Setup\n",
    "        self.w_0LS = nn.Parameter(torch.rand(1, requires_grad=True, dtype=torch.float))\n",
    "        self.w_0LF = nn.Parameter(torch.rand(1, requires_grad=True, dtype=torch.float))\n",
    "        self.w_0BD = nn.Parameter(torch.rand(1, requires_grad=True, dtype=torch.float))\n",
    "        self.w_eLS = nn.Parameter(torch.rand(1, requires_grad=True, dtype=torch.float))\n",
    "        self.w_eLF = nn.Parameter(torch.rand(1, requires_grad=True, dtype=torch.float))\n",
    "        self.w_eBD = nn.Parameter(torch.rand(1, requires_grad=True, dtype=torch.float))\n",
    "        self.w_ex  = nn.Parameter(torch.rand(1, requires_grad=True, dtype=torch.float))\n",
    "        self.w_0x  = nn.Parameter(torch.rand(1, requires_grad=True, dtype=torch.float))\n",
    "        self.var_x = nn.Parameter(torch.rand(1, requires_grad=True, dtype=torch.float))\n",
    "        self.w_LSx = nn.Parameter(torch.rand(1, requires_grad=True, dtype=torch.float)) \n",
    "        self.w_LFx = nn.Parameter(torch.rand(1, requires_grad=True, dtype=torch.float)) \n",
    "        self.w_BDx = nn.Parameter(torch.rand(1, requires_grad=True, dtype=torch.float)) \n",
    "        self.w_LSBD = nn.Parameter(torch.rand(1, requires_grad=True, dtype=torch.float)) \n",
    "        self.w_LFBD = nn.Parameter(torch.rand(1, requires_grad=True, dtype=torch.float)) \n",
    "        self.w_aBD  = nn.Parameter(torch.rand(1, requires_grad=True, dtype=torch.float))/2\n",
    "        self.w_aLS  = nn.Parameter(torch.rand(1, requires_grad=True, dtype=torch.float))/2\n",
    "        self.w_aLF  = nn.Parameter(torch.rand(1, requires_grad=True, dtype=torch.float))/2\n",
    "        \n",
    "        if args.cuda:\n",
    "            self.FloatTensor = torch.cuda.FloatTensor\n",
    "        else:\n",
    "            self.FloatTensor = torch.FloatTensor\n",
    "      \n",
    "    def init_pseudoinputs(self, pseudo_inputs):\n",
    "        \"\"\"\n",
    "        Adds and initialises additional layer for pseudoinput generation\n",
    "        pseudo_inputs: either random training data or None\n",
    "        \"\"\"\n",
    "        self.pseudo_inputs = pseudo_inputs\n",
    "        \n",
    "        if pseudo_inputs is None:\n",
    "            # initialise dummy inputs\n",
    "            if self.is_cuda:\n",
    "                self.dummy_inputs = torch.eye(self.num_pseudos).cuda()\n",
    "            else:\n",
    "                self.dummy_inputs = torch.eye(self.num_pseudos)\n",
    "            self.dummy_inputs.requires_grad = False\n",
    "            # initialise layers for learning pseudoinputs\n",
    "            self.pseudo_layer = nn.Linear(self.num_pseudos, self.batch_size, bias=False)\n",
    "            #default in experiment parser\n",
    "            self.pseudo_nonlin = nn.Hardtanh(min_val=0.0, max_val=1.0)\n",
    "        else:        \n",
    "            if self.is_cuda:\n",
    "                self.pseudo_inputs = self.pseudo_inputs.cuda()     \n",
    "            self.pseudo_inputs.requires_grad = False\n",
    "\n",
    "            \n",
    "    def log_vamp_zk(self, zk):\n",
    "        \"\"\"\n",
    "        Calculates log p(z_k) under VampPrior\n",
    "        \"\"\"\n",
    "        # generate pseudoinputs from diagonal tensor\n",
    "        if self.pseudo_inputs is None:\n",
    "            pseudo_x = self.pseudo_nonlin(self.pseudo_layer(self.dummy_inputs))\n",
    "        else:\n",
    "            pseudo_x = self.pseudo_inputs\n",
    "            \n",
    "        # calculate VampPrior\n",
    "        vamp_mu, vamp_logvar, _, _, _ = self.amortize(pseudo_x)\n",
    "        \n",
    "        # expand\n",
    "        zk_expanded = zk.unsqueeze(1) # (batch_size, 1,          z_size)\n",
    "        mus = vamp_mu.unsqueeze(0)    # (1,         num_pseudos, z_size)\n",
    "        logvars = vamp_logvar.unsqueeze(0)    # (1, num_pseudos, z_size)\n",
    "        \n",
    "        # calculate log p(z_k)\n",
    "        log_per_pseudo = log_normal_dist(zk_expanded, mus, logvars, dim=1) - math.log(self.num_pseudos)\n",
    "        # (batch_size,num_pseudos)\n",
    "        log_total = torch.logsumexp(log_per_pseudo, 0)\n",
    "        # (batch_size)\n",
    "        return log_total\n",
    "    \n",
    "    def reparameterize(self, mu, var):\n",
    "        \"\"\"\n",
    "        Samples z from a multivariate Gaussian with diagonal covariance matrix using the\n",
    "         reparameterization trick.\n",
    "        \"\"\"\n",
    "        std = var.sqrt()\n",
    "        eps = torch.randn_like(std)\n",
    "        z = eps * std + mu\n",
    "        return self.m(z)\n",
    "\n",
    "    def amortize(self, a):\n",
    "        '''\n",
    "        Calculate the parameters obtained from the existing models for the base \n",
    "        distribution of normalizing flows\n",
    "        \n",
    "        '''\n",
    "        mu = self.mu(a)\n",
    "        var = self.var(a)\n",
    "        u = self.amor_u(a).view(-1, self.num_flows, self.batch_size, 1)\n",
    "        w = self.amor_w(a).view(-1, self.num_flows, 1, self.batch_size)\n",
    "        b = self.amor_b(a).view(-1, self.num_flows, 1, 1)\n",
    "        return mu, var, u, w, b\n",
    "\n",
    "    def forward(self, a_LS, a_LF, a_BD, x):      \n",
    "        if self.is_cuda:\n",
    "            self.log_det_j_LS = torch.zeros([x.shape[0]]).cuda()\n",
    "            self.log_det_j_LF = torch.zeros([x.shape[0]]).cuda()\n",
    "            self.log_det_j_BD = torch.zeros([x.shape[0]]).cuda()\n",
    "        else:\n",
    "            self.log_det_j_LS = torch.zeros([x.shape[0]])\n",
    "            self.log_det_j_LF = torch.zeros([x.shape[0]])\n",
    "            self.log_det_j_BD = torch.zeros([x.shape[0]])\n",
    "\n",
    "        a_LS_1 = self.w_aLS*a_LS + self.w_0LS\n",
    "        a_LF_1 = self.w_aLF*a_LF + self.w_0LF\n",
    "        \n",
    "#         a_LS_1[a_LS_1 > 1] = 1\n",
    "#         a_LF_1[a_LF_1 > 1] = 1\n",
    "\n",
    "        # mean and variance of z\n",
    "        z_LF_mu, y_LF_var, u_LF, w_LF, b_LF = self.amortize(a_LF_1)\n",
    "        z_LF_var = y_LF_var + self.w_eLF**2\n",
    "        z_LF = self.reparameterize(z_LF_mu, z_LF_var)\n",
    "        z_LF = [z_LF]\n",
    "        \n",
    "        z_LS_mu, y_LS_var, u_LS, w_LS, b_LS = self.amortize(a_LS_1)\n",
    "        z_LS_var = y_LS_var + self.w_eLS**2    \n",
    "        z_LS = self.reparameterize(z_LS_mu, z_LS_var)\n",
    "        z_LS = [z_LS]  \n",
    "        \n",
    "        for k in range(self.num_flows):\n",
    "            flow_k_LS = getattr(self, 'flow_LS_' + str(k))\n",
    "            z_LSk, log_det_jacobian_LS = flow_k_LS(z_LS[k], u_LS[:, k, :, :], w_LS[:, k, :, :], b_LS[:, k, :, :])\n",
    "            z_LS.append(z_LSk)\n",
    "            self.log_det_j_LS = self.log_det_j_LS + log_det_jacobian_LS  \n",
    "            \n",
    "        for k in range(self.num_flows):\n",
    "            flow_k_LF = getattr(self, 'flow_LF_' + str(k))\n",
    "            z_LFk, log_det_jacobian_LF = flow_k_LF(z_LF[k], u_LF[:, k, :, :], w_LF[:, k, :, :], b_LF[:, k, :, :])\n",
    "            z_LF.append(z_LFk)\n",
    "            self.log_det_j_LF = self.log_det_j_LF + log_det_jacobian_LF\n",
    "\n",
    "        \n",
    "\n",
    "        z_LS_0 = z_LS[0]\n",
    "        z_LF_0 = z_LF[0]\n",
    "        z_LS_K = self.m(z_LS[-1])\n",
    "        z_LF_K = self.m(z_LF[-1])\n",
    "        \n",
    "        a_BD_1 = self.w_LSBD*z_LS_K.view(-1,self.batch_size) + self.w_LFBD*z_LF_K.view(-1,self.batch_size) + self.w_aBD*a_BD + self.w_0BD\n",
    "#         a_BD_1[a_BD_1 > 1] = 1\n",
    "        z_BD_mu, y_BD_var, u_BD, w_BD, b_BD = self.amortize(a_BD_1)\n",
    "        z_BD_var = y_BD_var + self.w_eBD**2\n",
    "        \n",
    "        \n",
    "        z_BD = self.reparameterize(z_BD_mu, z_BD_var)\n",
    "        z_BD = [z_BD]\n",
    "        \n",
    "        for k in range(self.num_flows):\n",
    "            flow_k_BD = getattr(self, 'flow_BD_' + str(k))\n",
    "            z_BDk, log_det_jacobian_BD = flow_k_BD(z_BD[k], u_BD[:, k, :, :], w_BD[:, k, :, :], b_BD[:, k, :, :])\n",
    "            z_BD.append(z_BDk)\n",
    "            self.log_det_j_BD = self.log_det_j_BD + log_det_jacobian_BD\n",
    "            \n",
    "        z_BD_0 = z_BD[0]\n",
    "        z_BD_K = self.m(z_BD[-1])\n",
    "        \n",
    "\n",
    "        BD_inner_1 = torch.exp(- self.w_LSBD - self.w_LFBD)*z_LS_K*z_LF_K + (1-z_LS_K)*(1-z_LF_K) + torch.exp(-self.w_LSBD)*z_LS_K*(1-z_LF_K)+ torch.exp(-self.w_LFBD)*z_LF_K*(1-z_LS_K)\n",
    "        BD_inner_2 = torch.exp(self.w_LSBD + self.w_LFBD)*z_LS_K*z_LF_K + (1-z_LS_K)*(1-z_LF_K) + torch.exp(self.w_LSBD)*z_LS_K*(1-z_LF_K)+ torch.exp(self.w_LFBD)*z_LF_K*(1-z_LS_K)\n",
    "        p_z_BD = (z_BD_K*(-torch.log(1+torch.exp(-(self.w_eBD**2)/2 - self.w_0BD - self.w_aBD*a_BD)*BD_inner_1))\n",
    "                 +(1-z_BD_K)*(-torch.log(1+torch.exp((self.w_eBD**2)/2 + self.w_0BD + self.w_aBD*a_BD)*BD_inner_2)))\n",
    "        p_z_LS = (z_LS_K*(-torch.log(1+torch.exp(-(self.w_eLS**2)/2 - self.w_0LS - self.w_aLS*a_LS)))\n",
    "                + (1-z_LS_K)*(-torch.log(1+torch.exp((self.w_eLS**2)/2 + self.w_0LS + self.w_aLS*a_LS))))\n",
    "        p_z_LF = (z_LF_K*(-torch.log(1+torch.exp(-(self.w_eLF**2)/2 - self.w_0LF - self.w_aLF*a_LF)))\n",
    "                + (1-z_LF_K)*(-torch.log(1+torch.exp((self.w_eLF**2)/2 + self.w_0LF + self.w_aLF*a_LF))))\n",
    "        \n",
    "        logp_zk = torch.mean(p_z_BD + p_z_LS + p_z_LF)\n",
    "    \n",
    "        log_p_xz = (- torch.log(x) - torch.log(torch.abs(torch.sqrt(self.w_ex))) \n",
    "                   - (torch.log(x)**2)/(2*(self.w_ex**2))\n",
    "                   + (torch.log(x)*(self.w_BDx*z_BD_K + self.w_LSx*z_LS_K + self.w_LFx*z_LF_K + self.w_0x))/(self.w_ex**2)\n",
    "                   - ((self.w_BDx**2)*z_BD_K + (self.w_LSx**2)*z_LS_K + (self.w_LFx**2)*z_LF_K + self.w_ex**2 + self.w_0x**2)/(2*(self.w_ex**2))\n",
    "                   - (self.w_BDx*self.w_LSx*z_BD_K*z_LS_K + self.w_BDx*self.w_LFx*z_BD_K*z_LF_K + self.w_LSx*self.w_LFx*z_LS_K*z_LF_K)/(self.w_ex**2)\n",
    "                   - (self.w_BDx*self.w_0x*z_BD_K + self.w_LSx*self.w_0x*z_LS_K + self.w_LFx*self.w_0x*z_LF_K)/(self.w_ex)**2)\n",
    "                    \n",
    "\n",
    "        return z_LS_mu, z_LS_var, z_LS_0, z_LS_K, self.log_det_j_LS, z_LF_mu, z_LF_var, z_LF_0, z_LF_K, self.log_det_j_LF,z_BD_mu, z_BD_var, z_BD_0, z_BD_K, self.log_det_j_BD,log_p_xz,logp_zk\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnum = np.floor(total_num/args.batch_size)\n",
    "rand_idx = np.array(random.sample(range(int(bnum)*args.batch_size),int(bnum)*args.batch_size))\n",
    "IND = torch.from_numpy(rand_idx[0:int(bnum*args.batch_size)])\n",
    "IND = IND.view(-1, args.batch_size)\n",
    "IND = IND.numpy()\n",
    "\n",
    "aLS = np.array(PLS).reshape([total_num])\n",
    "aLS = aLS[rand_idx[0:int(bnum*args.batch_size)]]\n",
    "aLS = torch.from_numpy(np.array(aLS)).float()\n",
    "aLS = aLS.view(-1,args.batch_size)\n",
    "\n",
    "aLF = np.array(PLF).reshape([total_num])\n",
    "aLF = aLF[rand_idx[0:int(bnum*args.batch_size)]]\n",
    "aLF = torch.from_numpy(np.array(aLF)).float()\n",
    "aLF = PLF.view(-1,args.batch_size)\n",
    "\n",
    "aBD = np.array(PBD).reshape([total_num])\n",
    "aBD = aBD[rand_idx[0:int(bnum*args.batch_size)]]\n",
    "aBD = torch.from_numpy(np.array(aBD)).float()\n",
    "aBD = aBD.view(-1,args.batch_size)\n",
    "\n",
    "obs = np.array(OBS).reshape([total_num])\n",
    "obs = obs[rand_idx[0:int(bnum*args.batch_size)]]\n",
    "obs = torch.from_numpy(np.array(obs)).float()\n",
    "obs = obs.view(-1,args.batch_size)\n",
    "\n",
    "LS = aLS.detach()\n",
    "LF = aLF.detach()\n",
    "BD = aBD.detach()\n",
    "OB = obs.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_PLS = torch.tensor(PLS.view(total_num).numpy().tolist().copy()).view(-1,1)\n",
    "test_PLF = torch.tensor(PLF.view(total_num).numpy().tolist().copy()).view(-1,1)\n",
    "test_PBD = torch.tensor(PBD.view(total_num).numpy().tolist().copy()).view(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('\\nMODEL SETTINGS: \\n', args, '\\n')\n",
    "# print(\"Random Seed: \", args.manual_seed)\n",
    "\n",
    "dataset = MyDataset(LS, LF, BD, OB)\n",
    "MyDataLoader = DataLoader(dataset=dataset,shuffle=True)\n",
    "model = GraphicalVINF(args)\n",
    "args.vampprior = False\n",
    "# if args.vampprior:\n",
    "#     load = torch.utils.data.DataLoader(MyDataLoader.dataset, batch_size=args.num_pseudos, shuffle=True)\n",
    "#     pseudo_inputs = None\n",
    "#     model.init_pseudoinputs(pseudo_inputs)\n",
    "if args.cuda:\n",
    "    print(\"Model on GPU\")\n",
    "    model.cuda()\n",
    "# print(model)\n",
    "opt = optim.RMSprop(model.parameters(), lr=args.learning_rate, momentum=0.9)\n",
    "loss = []\n",
    "t_loss = 1e-6\n",
    "epoch = 0\n",
    "t = time.time()\n",
    "\n",
    "TLoss = []\n",
    "epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# while epoch <= 10:\n",
    "\n",
    "for epoch in range(1, args.epochs + 1):\n",
    "    \n",
    "    \n",
    "    train_loss = np.zeros(len(MyDataLoader))\n",
    "    index = -1\n",
    "    i = 0\n",
    "    batch_size = args.batch_size\n",
    "    cuda = args.cuda\n",
    "\n",
    "    while t_loss > 0:\n",
    "        for aLS, aLF, aBD, obs in MyDataLoader: \n",
    "            index += 1\n",
    "            # Beta-annealing\n",
    "            beta = min(0.01 + ((epoch - 1) * (len(MyDataLoader)/args.batch_size) + index + 1) / args.epochs , 1)\n",
    "\n",
    "            if args.cuda:\n",
    "                aLS = aLS.cuda()\n",
    "                aLF = aLF.cuda()\n",
    "                aBD = aBD.cuda()\n",
    "                obs = obs.cuda()\n",
    "            aLS = aLS.view(-1,args.batch_size)\n",
    "            aLF = aLF.view(-1,args.batch_size)\n",
    "            aBD = aBD.view(-1,args.batch_size)\n",
    "            obs = obs.view(-1,args.batch_size)\n",
    "\n",
    "\n",
    "            #Pass throught the Graphical VI + flows  \n",
    "            q_LS_mu, q_LS_var, q_LS_0, q_LS_K, log_det_j_LS, q_LF_mu, q_LF_var, q_LF_0, q_LF_K, log_det_j_LF, q_BD_mu, q_BD_var, q_BD_0, q_BD_K, log_det_j_BD, log_p_xz, logp_zk = model(aLS, aLF, aBD, obs)\n",
    "\n",
    "            print('logp_zk: ', logp_zk)\n",
    "            print('log_p_xz: ', torch.mean(log_p_xz))\n",
    "            new_q_LS_K = q_LS_K.view(-1,1)\n",
    "            new_q_LF_K = q_LF_K.view(-1,1)\n",
    "            new_q_BD_K = q_BD_K.view(-1,1)\n",
    "            test_PLS[IND[i]] = new_q_LS_K\n",
    "            test_PLF[IND[i]] = new_q_LF_K\n",
    "            test_PBD[IND[i]] = new_q_BD_K\n",
    "\n",
    "\n",
    "            log_q_z0_LS = log_normal_dist(q_LS_0, mean=q_LS_mu, logvar=q_LS_var.log(), dim=1)\n",
    "            log_q_z0_LF = log_normal_dist(q_LF_0, mean=q_LF_mu, logvar=q_LF_var.log(), dim=1)\n",
    "            log_q_z0_BD = log_normal_dist(q_BD_0, mean=q_BD_mu, logvar=q_BD_var.log(), dim=1)\n",
    "\n",
    "            log_q_z0 = (log_q_z0_LS + log_q_z0_LF + log_q_z0_BD)/args.batch_size\n",
    "            print('log_q_z0: ',log_q_z0)\n",
    "\n",
    "            log_det_jacobians = log_det_j_LS + log_det_j_LF + log_det_j_BD\n",
    "            triplet_loss = nn.TripletMarginLoss(margin=1.0, p=2)\n",
    "            penalty_LS = triplet_loss(q_LS_K, aLS, aLF)\n",
    "            penalty_LF = triplet_loss(q_LF_K, aLF, aLS)\n",
    "\n",
    "\n",
    "            if args.anneal == \"std\":\n",
    "                #Equation (20)\n",
    "                kl = log_q_z0 - beta * logp_zk - log_det_jacobians #sum over batches\n",
    "                loss = beta * torch.abs(torch.mean(log_p_xz)) + kl + penalty_LS + penalty_LF\n",
    "            elif args.anneal == \"off\":\n",
    "                kl = torch.mean(log_q_z0 - logp_zk - log_det_jacobians) #sum over batches\n",
    "                loss = - torch.mean(log_p_xz) + kl + penalty_LS + penalty_LF\n",
    "            elif args.anneal == \"kl\":\n",
    "                kl = torch.mean(log_q_z0 - logp_zk - log_det_jacobians) #sum over batches\n",
    "                loss = - torch.mean(log_p_xz) + beta * kl + penalty_LS + penalty_LF\n",
    "        #     print('loss: ', loss)\n",
    "            # loss: log_q_0 - log_p_xz - log_p_zk - ldj\n",
    "\n",
    "\n",
    "        #Optimization step, Backpropagation\n",
    "            opt.zero_grad()\n",
    "            loss.backward(retain_graph=True)\n",
    "    #         torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
    "    #         for p in model.parameters():\n",
    "    #             p.data.add_(p.grad, alpha=-0.01)\n",
    "\n",
    "            train_loss[index] = loss.item()\n",
    "            opt.step()\n",
    "            t_loss = loss.item()\n",
    "            if index % args.log_interval == 0:\n",
    "                print('Epoch: {:3d} [({:2.0f}%)]'.format(\n",
    "                        epoch, 100. * index / len(MyDataLoader)))\n",
    "            i += 1\n",
    "\n",
    "        TLoss.append(train_loss.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
