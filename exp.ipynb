{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scalr Case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import argparse\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "import json\n",
    "import pathlib\n",
    "from torchinfo import summary\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "from PIL import Image\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='PyTorch Graphical VINF')\n",
    "\n",
    "parser.add_argument('-d', '--dataset', type=str, default='mnist', choices=['mnist'],\n",
    "                    metavar='DATASET',\n",
    "                    help='Dataset choice.')\n",
    "\n",
    "parser.add_argument('-nc', '--no_cuda', action='store_true', default=False,\n",
    "                    help='disables CUDA training')\n",
    "\n",
    "parser.add_argument('--manual_seed', type=int, help='manual seed, if not given resorts to random seed.')\n",
    "\n",
    "parser.add_argument('-li', '--log_interval', type=int, default=10, metavar='LOG_INTERVAL',\n",
    "                    help='how many batches to wait before logging training status')\n",
    "\n",
    "parser.add_argument('-od', '--out_dir', type=str, default='logs/', metavar='OUT_DIR',\n",
    "                    help='output directory for model snapshots etc.')\n",
    "\n",
    "fp = parser.add_mutually_exclusive_group(required=False)\n",
    "fp.add_argument('-te', '--testing', action='store_true', dest='testing',\n",
    "                help='evaluate on test set after training')\n",
    "fp.add_argument('-va', '--validation', action='store_false', dest='testing',\n",
    "                help='only evaluate on validation set')\n",
    "parser.set_defaults(testing=True)\n",
    "\n",
    "# optimization settings\n",
    "parser.add_argument('-e', '--epochs', type=int, default= 30, metavar='EPOCHS',\n",
    "                    help='number of epochs to train (default: 30)')\n",
    "parser.add_argument('-es', '--early_stopping_epochs', type=int, default=15, metavar='EARLY_STOPPING',\n",
    "                    help='number of early stopping epochs')\n",
    "\n",
    "parser.add_argument('-bs', '--batch_size', type=int, default=2000, metavar='BATCH_SIZE',\n",
    "                    help='input batch size for training (default: 100)')\n",
    "parser.add_argument('-lr', '--learning_rate', type=float, default=0.00001, metavar='LEARNING_RATE',\n",
    "                    help='learning rate')\n",
    "\n",
    "parser.add_argument('-a', '--anneal', type=str, default=\"std\", choices= [\"std\", \"off\", \"kl\"], help=\"beta annealing scheme\")\n",
    "parser.add_argument('--max_beta', type=float, default=1., metavar='MB',\n",
    "                    help='max beta for warm-up')\n",
    "parser.add_argument('--min_beta', type=float, default=0.0, metavar='MB',\n",
    "                    help='min beta for warm-up')\n",
    "parser.add_argument('-f', '--flow', type=str, default='planar', choices=['planar', 'NICE', 'NICE_MLP', 'real' ])\n",
    "parser.add_argument('-nf', '--num_flows', type=int, default=5,\n",
    "                    metavar='NUM_FLOWS', help='Number of flow layers, ignored in absence of flows')\n",
    "\n",
    "parser.add_argument('--z_size', type=int, default=1, metavar='ZSIZE',\n",
    "                    help='how many stochastic hidden units')\n",
    "\n",
    "\n",
    "parser.add_argument('-vp', '--vampprior', type=bool, default=True, metavar='VAMPPRIOR',\n",
    "                    help='choose whether to use VampPrior')\n",
    "\n",
    "parser.add_argument('--num_pseudos', type=int, default=100, metavar='NUM_PSEUDOS',\n",
    "                    help='number of pseudoinputs used for VampPrior')\n",
    "\n",
    "parser.add_argument('--data_as_pseudo', type=bool, default=True, metavar='data_as_pseudo',\n",
    "                    help='use random training data as pseudoinputs')\n",
    "\n",
    "# gpu/cpu\n",
    "parser.add_argument('--gpu_num', type=int, default=0, metavar='GPU', help='choose GPU to run on.')\n",
    "\n",
    "args = parser.parse_args(args=[])\n",
    "args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "\n",
    "if args.manual_seed is None:\n",
    "    args.manual_seed = 42\n",
    "random.seed(args.manual_seed)\n",
    "torch.manual_seed(args.manual_seed)\n",
    "np.random.seed(args.manual_seed)\n",
    "\n",
    "if args.cuda:\n",
    "    torch.cuda.set_device(args.gpu_num)\n",
    "args.data_as_pseudo = False\n",
    "\n",
    "args.batch_size = 200\n",
    "args.num_flows = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate the 'true' posterior and causal dependency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "total_num = 100000\n",
    "loc_LS, scale_LS = 0.5, 0.004\n",
    "pLS = np.random.logistic(loc_LS, scale_LS, total_num)\n",
    "\n",
    "loc_LF, scale_LF = 0.1, 0.003\n",
    "pLF = np.random.logistic(loc_LF, scale_LF, total_num)\n",
    "\n",
    "loc_BD, scale_BD = 0.8, 0.005\n",
    "pBD = np.random.logistic(loc_BD, scale_BD, total_num)\n",
    "\n",
    "count_LS, bins_LS, ignored_LS = plt.hist(pLS, bins=500)\n",
    "count_LF, bins_LF, ignored_LF = plt.hist(pLF, bins=500)\n",
    "count_BD, bins_BD, ignored_BD = plt.hist(pBD, bins=500)\n",
    "\n",
    "PLS = torch.from_numpy(pLS).float()\n",
    "\n",
    "PLF = torch.from_numpy(pLF).float()\n",
    "\n",
    "PBD = torch.from_numpy(pBD).float()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.8539), tensor(0.1344), tensor(0.5470))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PBD.max(),PLF.max(),PLS.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "PLS = PLS.view(-1,args.batch_size)\n",
    "PLF = PLF.view(-1,args.batch_size)\n",
    "PBD = PBD.view(-1,args.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_0LS = torch.tensor(0.2956)\n",
    "w_0LF = torch.tensor(0.6414)\n",
    "w_0BD = torch.tensor(0.2184)\n",
    "w_eLS = torch.tensor(0.0186)\n",
    "w_eLF = torch.tensor(0.9015)\n",
    "w_eBD = torch.tensor(0.1920)\n",
    "w_ex  = torch.tensor(0.0267)\n",
    "w_0x  = torch.tensor(0.5337)\n",
    "var_x = torch.tensor(0.1333)\n",
    "w_LSx = torch.tensor(0.2258)\n",
    "w_LFx = torch.tensor(0.6058)\n",
    "w_BDx = torch.tensor(0.3021)\n",
    "\n",
    "w_aBD  = torch.tensor(0.7886)\n",
    "w_aLS  = torch.tensor(0.4105)\n",
    "w_aLF  = torch.tensor(0.2108)\n",
    "\n",
    "w_gBD = torch.tensor(0.8761) \n",
    "w_LSBD = torch.tensor(0.9303)\n",
    "w_LFBD = torch.tensor(0.1363)\n",
    " \n",
    "w_LSBD = torch.tensor(0.2513)\n",
    "w_LFBD = torch.tensor(0.3612)\n",
    "\n",
    "m = nn.Sigmoid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnum = np.floor(total_num/args.batch_size)\n",
    "rand_idx = np.array(random.sample(range(int(bnum)*args.batch_size),int(bnum)*args.batch_size))\n",
    "IND = torch.from_numpy(rand_idx[0:int(bnum*args.batch_size)])\n",
    "IND = IND.view(-1, args.batch_size)\n",
    "IND = IND.numpy()\n",
    "\n",
    "aLS = np.array(PLS).reshape([total_num])\n",
    "aLS = aLS[rand_idx[0:int(bnum*args.batch_size)]]\n",
    "aLS = torch.from_numpy(np.array(aLS)).float()\n",
    "aLS = aLS.view(-1,args.batch_size)\n",
    "\n",
    "aLF = np.array(PLF).reshape([total_num])\n",
    "aLF = aLF[rand_idx[0:int(bnum*args.batch_size)]]\n",
    "aLF = torch.from_numpy(np.array(aLF)).float()\n",
    "aLF = aLF.view(-1,args.batch_size)\n",
    "\n",
    "aBD = np.array(PBD).reshape([total_num])\n",
    "aBD = aBD[rand_idx[0:int(bnum*args.batch_size)]]\n",
    "aBD = torch.from_numpy(np.array(aBD)).float()\n",
    "aBD = aBD.view(-1,args.batch_size)\n",
    "\n",
    "\n",
    "LS = aLS.detach()\n",
    "LF = aLF.detach()\n",
    "BD = aBD.detach()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, aLS, aLF,aBD):\n",
    "        self.aLS, self.aLF, self.aBD =  aLS, aLF, aBD\n",
    "    def __getitem__(self, idx):\n",
    "        return (self.aLS[idx], self.aLF[idx], self.aBD[idx])\n",
    "    def __len__(self):\n",
    "        return self.aLS.size(0)\n",
    "\n",
    "dataset = MyDataset(LS, LF, BD)\n",
    "MyDataLoader = DataLoader(dataset=dataset,shuffle=True)\n",
    "\n",
    "PLS1_1 = PLS.view(-1,1).detach()\n",
    "PLF1_1 = PLF.view(-1,1).detach()\n",
    "PBD1_1 = PBD.reshape(-1,1).detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reparameterize(mu, var):\n",
    "    \"\"\"\n",
    "    Samples z from a multivariate Gaussian with diagonal covariance matrix using the\n",
    "     reparameterization trick.\n",
    "    \"\"\"\n",
    "    m = nn.Sigmoid()\n",
    "    std = var.sqrt()\n",
    "    eps = torch.randn_like(std)\n",
    "    z = eps * std + mu\n",
    "#     z_max = z.max()\n",
    "#     z_min = z.min()\n",
    "#     z = (z-z_min)/(z_max - z_min)\n",
    "    return m(z)\n",
    "    # q_d \\in [0,1]\n",
    "\n",
    "def f(y):\n",
    "    return torch.log(1-torch.exp(-y))\n",
    "\n",
    "def flow_k(q, u, w, b):\n",
    "\n",
    "    h = nn.Tanh()\n",
    "    m = nn.Sigmoid()\n",
    "    \"\"\"\n",
    "    Computes the following transformation:\n",
    "    z' = z + u h( w^T z + b)\n",
    "    Input shapes:\n",
    "    shape u = (batch_size, z_size, 1)\n",
    "    shape w = (batch_size, 1, z_size)\n",
    "    shape b = (batch_size, 1, 1)\n",
    "    shape z = (batch_size, z_size).\n",
    "    \"\"\"\n",
    "    # Equation (10)\n",
    "    q = q.unsqueeze(2)\n",
    "    prod = torch.bmm(w, q) + b\n",
    "    f_q = q + u * h(prod) # this is a 3d vector\n",
    "    f_q = f_q.squeeze(2) # this is a 2d vector\n",
    "    \n",
    "\n",
    "    # compute logdetJ\n",
    "    # Equation (11)\n",
    "    psi = w * (1 - h(prod) ** 2)  # w * h'(prod)\n",
    "    # Equation (12)\n",
    "    log_det_jacobian = torch.log(torch.abs(1 + torch.bmm(psi, u)))\n",
    "    log_det_jacobian = log_det_jacobian.squeeze(2).squeeze(1)\n",
    "\n",
    "    return f_q, log_det_jacobian\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for aLS, aLF, aBD in MyDataLoader: \n",
    "    mu = nn.Sequential(nn.Linear(args.batch_size, args.batch_size),\n",
    "                   nn.Hardtanh(min_val=-2, max_val=2))\n",
    "    var = nn.Sequential(nn.Linear(args.batch_size, args.batch_size),\n",
    "                        nn.Softplus(),\n",
    "                        nn.Hardtanh(min_val=1, max_val=5))\n",
    "    amor_u = nn.Sequential(nn.Linear(args.batch_size, args.num_flows*args.batch_size),\n",
    "                           nn.Hardtanh(min_val=-1, max_val=1))\n",
    "    amor_w = nn.Sequential(nn.Linear(args.batch_size, args.num_flows*args.batch_size),\n",
    "                           nn.Hardtanh(min_val=-1, max_val=1))\n",
    "    amor_b = nn.Sequential(nn.Linear(args.batch_size, args.num_flows),\n",
    "                           nn.Hardtanh(min_val=-1, max_val=1))\n",
    "\n",
    "\n",
    "    aLS = aLS.view(-1,args.batch_size)\n",
    "    aLF = aLF.view(-1,args.batch_size)\n",
    "    aBD = aBD.view(-1,args.batch_size)\n",
    "    \n",
    "    PLS_1 = w_aLS*aLS + w_0LS\n",
    "    PLF_1 = w_aLF*aLF + w_0LF\n",
    "    q_LS_mu = mu(PLS_1) \n",
    "    y_LS_var = var(PLS_1)\n",
    "    u_LS = amor_u(PLS_1).view(-1, args.num_flows, args.batch_size, 1)\n",
    "    w_LS = amor_w(PLS_1).view(-1, args.num_flows, 1, args.batch_size)\n",
    "    b_LS = amor_b(PLS_1).view(-1, args.num_flows, 1, 1)\n",
    "    q_LS_var = y_LS_var + w_eLS**2    \n",
    "    q_LS = reparameterize(q_LS_mu, q_LS_var)\n",
    "    q_LS = [q_LS]  \n",
    "    \n",
    "    q_LF_mu = mu(PLF_1) \n",
    "    y_LF_var = var(PLF_1)\n",
    "    u_LF = amor_u(PLF_1).view(-1, args.num_flows, args.batch_size, 1)\n",
    "    w_LF = amor_w(PLF_1).view(-1, args.num_flows, 1, args.batch_size)\n",
    "    b_LF = amor_b(PLF_1).view(-1, args.num_flows, 1, 1)\n",
    "    q_LF_var = y_LF_var + w_eLF**2\n",
    "    q_LF = reparameterize(q_LF_mu, q_LF_var)\n",
    "    q_LF = [q_LF]\n",
    "\n",
    "    log_det_j_LS = 0.\n",
    "    log_det_j_LF = 0.\n",
    "    log_det_j_BD = 0.\n",
    "\n",
    "\n",
    "    for k in range(args.num_flows):\n",
    "        q_LFk, log_det_jacobian_LF = flow_k(q_LF[k], u_LF[:, k, :, :], w_LF[:, k, :, :], b_LF[:, k, :, :])\n",
    "        q_LF.append(q_LFk)\n",
    "        log_det_j_LF = log_det_j_LF + log_det_jacobian_LF\n",
    "\n",
    "    q_LF_0 = q_LF[0]\n",
    "    q_LF_K = m(q_LF[-1])\n",
    "\n",
    "    for k in range(args.num_flows):\n",
    "        q_LSk, log_det_jacobian_LS = flow_k(q_LS[k], u_LS[:, k, :, :], w_LS[:, k, :, :], b_LS[:, k, :, :])\n",
    "        q_LS.append(q_LSk)\n",
    "        log_det_j_LS = log_det_j_LS + log_det_jacobian_LS \n",
    "    q_LS_0 = q_LS[0]\n",
    "    q_LS_K = m(q_LS[-1])\n",
    "\n",
    "\n",
    "\n",
    "    PBD_1 = w_LSBD*q_LS_K + w_LFBD*q_LF_K + w_aBD*aBD + w_0BD\n",
    "\n",
    "    q_BD_mu = mu(PBD_1) \n",
    "    y_BD_var = var(PBD_1)\n",
    "    u_BD = amor_u(PBD_1).view(-1, args.num_flows, args.batch_size, 1)\n",
    "    w_BD = amor_w(PBD_1).view(-1, args.num_flows, 1, args.batch_size)\n",
    "    b_BD = amor_b(PBD_1).view(-1, args.num_flows, 1, 1)\n",
    "\n",
    "\n",
    "    q_BD_var = y_BD_var + w_eBD**2\n",
    "    q_BD = reparameterize(q_BD_mu, q_BD_var)\n",
    "    q_BD = [q_BD]\n",
    "\n",
    "    for k in range(args.num_flows):\n",
    "        q_BDk, log_det_jacobian_BD = flow_k(q_BD[k], u_BD[:, k, :, :], w_BD[:, k, :, :], b_BD[:, k, :, :])\n",
    "        q_BD.append(q_BDk)\n",
    "        log_det_j_BD = log_det_j_BD + log_det_jacobian_BD\n",
    "\n",
    "    q_BD_0 = q_BD[0]\n",
    "    q_BD_K = m(q_BD[-1])\n",
    "    \n",
    "    PLS1_1[IND[i]] = q_LS_K.view(-1,1)\n",
    "    PLF1_1[IND[i]] = q_LF_K.view(-1,1)\n",
    "    PBD1_1[IND[i]] = q_BD_K.view(-1,1)\n",
    "\n",
    "    \n",
    "    i = i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# PLS_1 = w_aLS*PLS + w_0LS\n",
    "# PLF_1 = w_aLF*PLF + w_0LF\n",
    "\n",
    "\n",
    "# q_LS_mu = mu(PLS_1) \n",
    "# y_LS_var = var(PLS_1)\n",
    "# u_LS = amor_u(PLS_1).view(-1, args.num_flows, args.batch_size, 1)\n",
    "# w_LS = amor_w(PLS_1).view(-1, args.num_flows, 1, args.batch_size)\n",
    "# b_LS = amor_b(PLS_1).view(-1, args.num_flows, 1, 1)\n",
    "# q_LS_var = y_LS_var + w_eLS**2    \n",
    "# q_LS = reparameterize(q_LS_mu, q_LS_var)\n",
    "# q_LS = [q_LS]  \n",
    "\n",
    "\n",
    "# q_LF_mu = mu(PLF_1) \n",
    "# y_LF_var = var(PLF_1)\n",
    "# u_LF = amor_u(PLF_1).view(-1, args.num_flows, args.batch_size, 1)\n",
    "# w_LF = amor_w(PLF_1).view(-1, args.num_flows, 1, args.batch_size)\n",
    "# b_LF = amor_b(PLF_1).view(-1, args.num_flows, 1, 1)\n",
    "# q_LF_var = y_LF_var + w_eLF**2\n",
    "# q_LF = reparameterize(q_LF_mu, q_LF_var)\n",
    "# q_LF = [q_LF]\n",
    "\n",
    "# log_det_j_LS = 0.\n",
    "# log_det_j_LF = 0.\n",
    "# log_det_j_BD = 0.\n",
    "\n",
    "\n",
    "# for k in range(args.num_flows):\n",
    "#     q_LFk, log_det_jacobian_LF = flow_k(q_LF[k], u_LF[:, k, :, :], w_LF[:, k, :, :], b_LF[:, k, :, :])\n",
    "#     q_LF.append(q_LFk)\n",
    "#     log_det_j_LF = log_det_j_LF + log_det_jacobian_LF\n",
    "    \n",
    "# q_LF_0 = q_LF[0]\n",
    "# q_LF_K = m(q_LF[-1])\n",
    "\n",
    "# for k in range(args.num_flows):\n",
    "#     q_LSk, log_det_jacobian_LS = flow_k(q_LS[k], u_LS[:, k, :, :], w_LS[:, k, :, :], b_LS[:, k, :, :])\n",
    "#     q_LS.append(q_LSk)\n",
    "#     log_det_j_LS = log_det_j_LS + log_det_jacobian_LS \n",
    "# q_LS_0 = q_LS[0]\n",
    "# q_LS_K = m(q_LS[-1])\n",
    "\n",
    "# PBD_1 = w_LSBD*q_LS_K + w_LFBD*q_LF_K + w_aBD*PBD + w_0BD\n",
    "\n",
    "# q_BD_mu = mu(PBD_1) \n",
    "# y_BD_var = var(PBD_1)\n",
    "# u_BD = amor_u(PBD_1).view(-1, args.num_flows, args.batch_size, 1)\n",
    "# w_BD = amor_w(PBD_1).view(-1, args.num_flows, 1, args.batch_size)\n",
    "# b_BD = amor_b(PBD_1).view(-1, args.num_flows, 1, 1)\n",
    "\n",
    "\n",
    "# q_BD_var = y_BD_var + w_eBD**2\n",
    "# q_BD = reparameterize(q_BD_mu, q_BD_var)\n",
    "# q_BD = [q_BD]\n",
    "\n",
    "# for k in range(args.num_flows):\n",
    "#     q_BDk, log_det_jacobian_BD = flow_k(q_BD[k], u_BD[:, k, :, :], w_BD[:, k, :, :], b_BD[:, k, :, :])\n",
    "# #     q_BDk = m(q_BDk)\n",
    "#     q_BD.append(q_BDk)\n",
    "#     log_det_j_BD = log_det_j_BD + log_det_jacobian_BD\n",
    "\n",
    "# q_BD_0 = q_BD[0]\n",
    "# q_BD_K = m(q_BD[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "PLS1_1 = PLS1_1.detach()\n",
    "PLF1_1 = PLF1_1.detach()\n",
    "PBD1_1 = PBD1_1.detach()\n",
    "h_sum = w_LSx*PLS1_1 + w_LFx*PLF1_1 + w_BDx*PBD1_1 + w_0x\n",
    "h_sum = h_sum.detach()\n",
    "OBS = np.random.lognormal(h_sum.numpy(),w_ex)\n",
    "OBS_max = np.max(OBS)\n",
    "OBS_min = np.min(OBS)\n",
    "OBS = (OBS - OBS_min)/(OBS_max - OBS_min)\n",
    "OBS[OBS == 0] = OBS[OBS > 0].min()\n",
    "OBS = torch.from_numpy(OBS).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Planar(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Planar, self).__init__()\n",
    "        self.h = nn.Tanh()\n",
    "\n",
    "    def forward(self, z, u, w, b):\n",
    "        \"\"\"\n",
    "        Computes the following transformation:\n",
    "        z' = z + u h( w^T z + b)\n",
    "        Input shapes:\n",
    "        shape u = (batch_size, 1, 1)\n",
    "        shape w = (batch_size, 1, 1)\n",
    "        shape b = (batch_size, 1, 1)\n",
    "        shape z = (batch_size, 1).\n",
    "        \"\"\"\n",
    "\n",
    "        # Equation (10)\n",
    "        z = z.unsqueeze(2)\n",
    "        prod = torch.bmm(w, z) + b\n",
    "        f_z = z + u * self.h(prod) # this is a 3d vector\n",
    "        f_z = f_z.squeeze(2) # this is a 2d vector\n",
    "#         print('w: ',w)\n",
    "#         print('prod: ', prod)\n",
    "#         print('f_z: ',f_z)\n",
    "\n",
    "        # compute logdetJ\n",
    "        # Equation (11)\n",
    "        psi = w * (1 - self.h(prod) ** 2)  # w * h'(prod)\n",
    "#         print('psi: ',psi)\n",
    "        # Equation (12)\n",
    "        log_det_jacobian = torch.log(torch.abs(1 + torch.bmm(psi, u)))\n",
    "        log_det_jacobian = log_det_jacobian.squeeze(2).squeeze(1)\n",
    "\n",
    "\n",
    "        return f_z, log_det_jacobian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 用Dataset封装数据集，仅做示范，实际可直接用TensorDataset封装\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, aLS, aLF,aBD,obs):\n",
    "        self.aLS, self.aLF, self.aBD, self.obs =  aLS, aLF, aBD, obs\n",
    "    #定义初始化变量\n",
    "    def __getitem__(self, idx):\n",
    "        return (self.aLS[idx], self.aLF[idx], self.aBD[idx], self.obs[idx])\n",
    "    #定义每次取出的对应数值\n",
    "    def __len__(self):\n",
    "        return self.aLS.size(0)\n",
    "    #定义tensor的总长度\n",
    "# 2. 用DataLoader定义数据批量迭代器\n",
    "\n",
    "def log_normal_dist(x, mean, logvar, dim):\n",
    "    log_norm = -0.5 * (logvar + (x - mean) * (x - mean) * logvar.exp().reciprocal()) \n",
    "\n",
    "    return torch.sum(log_norm, dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphicalVINF(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(GraphicalVINF, self).__init__()\n",
    "\n",
    "        # extract model settings from args\n",
    "        self.batch_size = args.batch_size\n",
    "        self.is_cuda = args.cuda\n",
    "        self.log_det_j_LS = 0.\n",
    "        self.log_det_j_LF = 0.\n",
    "        self.log_det_j_BD = 0.\n",
    "        self.num_pseudos = args.num_pseudos # for initialising pseudoinputs\n",
    "        \n",
    "        flowLS = Planar # For normalizing flow\n",
    "        flowLF = Planar\n",
    "        flowBD = Planar\n",
    "        self.num_flows = args.num_flows\n",
    "        self.m = nn.Sigmoid()\n",
    "        \n",
    "        # Normalizing flow layers\n",
    "        for k in range(self.num_flows):\n",
    "            flow_k_LS = flowLS()\n",
    "            self.add_module('flow_LS_' + str(k), flow_k_LS)\n",
    "        for k in range(self.num_flows):\n",
    "            flow_k_LF = flowLF()\n",
    "            self.add_module('flow_LF_' + str(k), flow_k_LF)\n",
    "        for k in range(self.num_flows):\n",
    "            flow_k_BD = flowBD()\n",
    "            self.add_module('flow_BD_' + str(k), flow_k_BD)\n",
    "        \n",
    "        \n",
    "        # Paramters in normalizing flows    \n",
    "        self.mu_LS = nn.Sequential(nn.Linear(self.batch_size, self.batch_size),\n",
    "                                   nn.Hardtanh(min_val=-1, max_val=1))\n",
    "        self.var_LS = nn.Sequential(nn.Linear(self.batch_size, self.batch_size),\n",
    "                                    nn.Softplus(),\n",
    "                                    nn.Hardtanh(min_val=1, max_val=5))\n",
    "        self.amor_u_LS = nn.Sequential(nn.Linear(self.batch_size, self.num_flows * self.batch_size),\n",
    "                                       nn.Hardtanh(min_val=-1, max_val=1))\n",
    "        self.amor_w_LS = nn.Sequential(nn.Linear(self.batch_size, self.num_flows * self.batch_size),\n",
    "                                       nn.Hardtanh(min_val=-1, max_val=1))\n",
    "        self.amor_b_LS = nn.Sequential(nn.Linear(self.batch_size, self.num_flows),\n",
    "                                       nn.Hardtanh(min_val=-1, max_val=1))\n",
    "\n",
    "        self.mu_LF = nn.Sequential(nn.Linear(self.batch_size, self.batch_size),\n",
    "                                   nn.Hardtanh(min_val=-1, max_val=1))\n",
    "        self.var_LF = nn.Sequential(nn.Linear(self.batch_size, self.batch_size),\n",
    "                                    nn.Softplus(),\n",
    "                                    nn.Hardtanh(min_val=1, max_val=5))\n",
    "        self.amor_u_LF = nn.Sequential(nn.Linear(self.batch_size, self.num_flows * self.batch_size),\n",
    "                                       nn.Hardtanh(min_val=-1, max_val=1))\n",
    "        self.amor_w_LF = nn.Sequential(nn.Linear(self.batch_size, self.num_flows * self.batch_size),\n",
    "                                       nn.Hardtanh(min_val=-1, max_val=1))\n",
    "        self.amor_b_LF = nn.Sequential(nn.Linear(self.batch_size, self.num_flows),\n",
    "                                       nn.Hardtanh(min_val=-1, max_val=1))\n",
    "\n",
    "        self.mu_BD = nn.Sequential(nn.Linear(self.batch_size, self.batch_size),\n",
    "                                   nn.Hardtanh(min_val=-1, max_val=1))\n",
    "        self.var_BD = nn.Sequential(nn.Linear(self.batch_size, self.batch_size),\n",
    "                                    nn.Softplus(),\n",
    "                                    nn.Hardtanh(min_val=1, max_val=5))\n",
    "        self.amor_u_BD = nn.Sequential(nn.Linear(self.batch_size, self.num_flows * self.batch_size),\n",
    "                                       nn.Hardtanh(min_val=-1, max_val=1))\n",
    "        self.amor_w_BD = nn.Sequential(nn.Linear(self.batch_size, self.num_flows * self.batch_size),\n",
    "                                       nn.Hardtanh(min_val=-1, max_val=1))\n",
    "        self.amor_b_BD = nn.Sequential(nn.Linear(self.batch_size, self.num_flows),\n",
    "                                       nn.Hardtanh(min_val=-1, max_val=1))\n",
    "\n",
    "        #nn.Softplus() = log(1+exp(x))\n",
    "        #Amortized flow parameters\n",
    "        # Parameters Setup\n",
    "        self.w_0LS = nn.Parameter(torch.rand(1, requires_grad=True, dtype=torch.float))\n",
    "        self.w_0LF = nn.Parameter(torch.rand(1, requires_grad=True, dtype=torch.float))\n",
    "        self.w_0BD = nn.Parameter(torch.rand(1, requires_grad=True, dtype=torch.float))\n",
    "        self.w_eLS = nn.Parameter(torch.rand(1, requires_grad=True, dtype=torch.float))\n",
    "        self.w_eLF = nn.Parameter(torch.rand(1, requires_grad=True, dtype=torch.float))\n",
    "        self.w_eBD = nn.Parameter(torch.rand(1, requires_grad=True, dtype=torch.float))\n",
    "        self.w_ex  = nn.Parameter(torch.rand(1, requires_grad=True, dtype=torch.float))\n",
    "        self.w_0x  = nn.Parameter(torch.rand(1, requires_grad=True, dtype=torch.float))\n",
    "        self.w_LSx = nn.Parameter(torch.rand(1, requires_grad=True, dtype=torch.float)) \n",
    "        self.w_LFx = nn.Parameter(torch.rand(1, requires_grad=True, dtype=torch.float)) \n",
    "        self.w_LSBD = nn.Parameter(torch.rand(1, requires_grad=True, dtype=torch.float)) \n",
    "        self.w_LFBD = nn.Parameter(torch.rand(1, requires_grad=True, dtype=torch.float)) \n",
    "        self.w_BDx = nn.Parameter(torch.rand(1, requires_grad=True, dtype=torch.float)) \n",
    "        self.w_aBD  = nn.Parameter(torch.rand(1, requires_grad=True, dtype=torch.float))\n",
    "        self.w_aLS  = nn.Parameter(torch.rand(1, requires_grad=True, dtype=torch.float))\n",
    "        self.w_aLF  = nn.Parameter(torch.rand(1, requires_grad=True, dtype=torch.float))\n",
    "        self.w_gBD = nn.Parameter(torch.rand(1, requires_grad=True, dtype=torch.float)) \n",
    "        self.t_gBD = nn.Parameter(torch.rand(1, requires_grad=True, dtype=torch.float))\n",
    "        self.t_aBD = nn.Parameter(torch.rand(1, requires_grad=True, dtype=torch.float))\n",
    "        self.t_aLS = nn.Parameter(torch.rand(1, requires_grad=True, dtype=torch.float))\n",
    "        self.t_aLF = nn.Parameter(torch.rand(1, requires_grad=True, dtype=torch.float))        \n",
    "\n",
    "        self.min_thres = 0.1\n",
    "        self.max_thres = 0.5\n",
    "        if args.cuda:\n",
    "            self.FloatTensor = torch.cuda.FloatTensor\n",
    "        else:\n",
    "            self.FloatTensor = torch.FloatTensor\n",
    "          \n",
    "    def reparameterize(self, mu, var):\n",
    "        \"\"\"\n",
    "        Samples z from a multivariate Gaussian with diagonal covariance matrix using the\n",
    "         reparameterization trick.\n",
    "        \"\"\"\n",
    "        std = var.sqrt()\n",
    "        eps = torch.randn_like(std)\n",
    "        z = eps * std + mu\n",
    "#         z_max = z.max()\n",
    "#         z_min = z.min()\n",
    "#         z = (z-z_min)/(z_max - z_min)\n",
    "        return self.m(z)\n",
    "\n",
    "    def f(self,y):\n",
    "        return torch.log(1-torch.exp(-y))\n",
    "        \n",
    "    def forward(self, a_LS, a_LF, a_BD, x):      \n",
    "        if self.is_cuda:\n",
    "            self.log_det_j_LS = torch.zeros([x.shape[0]]).cuda()\n",
    "            self.log_det_j_LF = torch.zeros([x.shape[0]]).cuda()\n",
    "            self.log_det_j_BD = torch.zeros([x.shape[0]]).cuda()\n",
    "        else:\n",
    "            self.log_det_j_LS = torch.zeros([x.shape[0]])\n",
    "            self.log_det_j_LF = torch.zeros([x.shape[0]])\n",
    "            self.log_det_j_BD = torch.zeros([x.shape[0]])\n",
    "\n",
    "        a_LS_1 = self.w_aLS*a_LS + self.w_0LS\n",
    "        a_LF_1 = self.w_aLF*a_LF + self.w_0LF\n",
    "        \n",
    "#         a_LS_1_max = a_LS_1.max()\n",
    "#         a_LS_1_min = a_LS_1.min()\n",
    "        a_LS_1 = self.m(a_LS_1)\n",
    "#     (a_LS_1 - a_LS_1_min)/(a_LS_1_max - a_LS_1_min)\n",
    "        \n",
    "#         a_LF_1_max = a_LF_1.max()\n",
    "#         a_LF_1_min = a_LF_1.min()\n",
    "        a_LF_1 = self.m(a_LF_1)\n",
    "#     (a_LF_1 - a_LF_1_min)/(a_LF_1_max - a_LF_1_min)\n",
    "        \n",
    "        \n",
    "        q_LS_mu = self.mu_LS(a_LS_1) \n",
    "        y_LS_var = self.var_LS(a_LS_1)\n",
    "        u_LS = self.amor_u_LS(a_LS_1).view(-1, self.num_flows, self.batch_size, 1)\n",
    "        w_LS = self.amor_w_LS(a_LS_1).view(-1, self.num_flows, 1, self.batch_size)\n",
    "        b_LS = self.amor_b_LS(a_LS_1).view(-1, self.num_flows, 1, 1)\n",
    "        q_LS_var = y_LS_var + self.w_eLS**2    \n",
    "        q_LS = self.reparameterize(q_LS_mu, q_LS_var)\n",
    "        q_LS = [q_LS]  \n",
    "\n",
    "        q_LF_mu = self.mu_LF(a_LS_1) \n",
    "        y_LF_var = self.var_LF(a_LS_1)\n",
    "        u_LF = self.amor_u_LF(a_LS_1).view(-1, self.num_flows, self.batch_size, 1)\n",
    "        w_LF = self.amor_w_LF(a_LS_1).view(-1, self.num_flows, 1, self.batch_size)\n",
    "        b_LF = self.amor_b_LF(a_LS_1).view(-1, self.num_flows, 1, 1)\n",
    "        q_LF_var = y_LF_var + self.w_eLF**2    \n",
    "        q_LF = self.reparameterize(q_LS_mu, q_LS_var)\n",
    "        q_LF = [q_LF]  \n",
    "\n",
    "        for k in range(self.num_flows):\n",
    "            flow_k_LS = getattr(self, 'flow_LS_' + str(k))\n",
    "            q_LSk, log_det_jacobian_LS = flow_k_LS(q_LS[k], u_LS[:, k, :, :], w_LS[:, k, :, :], b_LS[:, k, :, :])\n",
    "#             LS_min = q_LSk.min()\n",
    "#             LS_max = q_LSk.max()\n",
    "            q_LSk = self.m(q_LSk)\n",
    "#     (q_LSk - LS_min)/(LS_max - LS_min)\n",
    "            q_LS.append(q_LSk)\n",
    "            self.log_det_j_LS = self.log_det_j_LS + log_det_jacobian_LS  \n",
    "            \n",
    "        for k in range(self.num_flows):\n",
    "            flow_k_LF = getattr(self, 'flow_LF_' + str(k))\n",
    "            q_LFk, log_det_jacobian_LF = flow_k_LF(q_LF[k], u_LF[:, k, :, :], w_LF[:, k, :, :], b_LF[:, k, :, :])\n",
    "#             LF_min = q_LFk.min()\n",
    "#             LF_max = q_LFk.max()\n",
    "            q_LFk = self.m(q_LFk)\n",
    "#     (q_LFk - LF_min)/(LF_max - LF_min)\n",
    "            q_LF.append(q_LFk)\n",
    "            self.log_det_j_LF = self.log_det_j_LF + log_det_jacobian_LF\n",
    "\n",
    "        q_LS_0 = q_LS[0]\n",
    "        q_LF_0 = q_LF[0]\n",
    "        q_LS_K = q_LS[-1]\n",
    "        q_LF_K = q_LF[-1]\n",
    "        \n",
    "        g = torch.ones([self.batch_size,1])\n",
    "        g[(q_LF_K.view(self.batch_size,-1) <= self.min_thres) & (q_LS_K.view(self.batch_size,-1) <= self.min_thres)] = 0\n",
    "        g[(q_LF_K.view(self.batch_size,-1) >= self.max_thres) & (q_LS_K.view(self.batch_size,-1) >= self.max_thres)] = 0\n",
    "        g = g.view(a_LS.size(0),a_LS.size(1))\n",
    "\n",
    "        a_BD_1 = self.w_gBD*g + self.w_aBD*a_BD + self.w_0BD\n",
    "#         a_BD_1_max = a_BD_1.max()\n",
    "#         a_BD_1_min = a_BD_1.min()\n",
    "        a_BD_1 = self.m(a_BD_1)\n",
    "#     (a_BD_1 - a_BD_1_min)/(a_BD_1_max - a_BD_1_min)\n",
    "        \n",
    "        q_BD_mu = self.mu_BD(a_BD_1) \n",
    "        y_BD_var = self.var_BD(a_BD_1)\n",
    "        u_BD = self.amor_u_BD(a_BD_1).view(-1, self.num_flows, self.batch_size, 1)\n",
    "        w_BD = self.amor_w_BD(a_BD_1).view(-1, self.num_flows, 1, self.batch_size)\n",
    "        b_BD = self.amor_b_BD(a_BD_1).view(-1, self.num_flows, 1, 1)\n",
    "        q_BD_var = y_BD_var + self.w_eBD**2    \n",
    "        q_BD = self.reparameterize(q_BD_mu, q_BD_var)\n",
    "        q_BD = [q_BD]  \n",
    "\n",
    "\n",
    "        for k in range(self.num_flows):\n",
    "            flow_k_BD = getattr(self, 'flow_BD_' + str(k))\n",
    "            q_BDk, log_det_jacobian_BD = flow_k_BD(q_BD[k], u_BD[:, k, :, :], w_BD[:, k, :, :], b_BD[:, k, :, :])\n",
    "#             BD_min = q_BDk.min()\n",
    "#             BD_max = q_BDk.max()\n",
    "            q_BDk = self.m(q_BDk)\n",
    "#     (q_BDk - BD_min)/(BD_max - BD_min)\n",
    "            q_BD.append(q_BDk)\n",
    "            self.log_det_j_BD = self.log_det_j_BD + log_det_jacobian_BD \n",
    "        q_BD_0 = q_BD[0]\n",
    "        q_BD_K = q_BD[-1]\n",
    "        \n",
    "        \n",
    "\n",
    "#         E_log_p_z_BD = q_BD_K*(-torch.log(1+((torch.exp(-self.w_gBD)*p_g_1 + p_g_0)*torch.exp(self.w_eBD**2/2 - self.w_aBD*a_BD - self.w_0BD)))) + (1-q_BD_K)*(-torch.log(1+((torch.exp(self.w_gBD)*p_g_1 + p_g_0)*torch.exp(self.w_eBD**2/2 + self.w_aBD*a_BD + self.w_0BD))))\n",
    "#         E_log_p_z_LS = q_LS_K*(-torch.log(1+torch.exp(-self.w_0LS - self.w_aLS*a_LS + self.w_eLS**2/2))) + (1-q_LS_K)*(-torch.log(1+torch.exp(self.w_0LS + self.w_aLS*a_LS + self.w_eLS**2/2)))\n",
    "#         E_log_p_z_LF = q_LF_K*(-torch.log(1+torch.exp(-self.w_0LF - self.w_aLF*a_LF + self.w_eLF**2/2))) + (1-q_LF_K)*(-torch.log(1+torch.exp(self.w_0LF + self.w_aLF*a_LF + self.w_eLF**2/2)))\n",
    "\n",
    "        \n",
    "    \n",
    "#         BD_inner_1 = torch.exp(- self.w_LSBD - self.w_LFBD)*q_LS_K*q_LF_K + (1-q_LS_K)*(1-q_LF_K) + torch.exp(-self.w_LSBD)*q_LS_K*(1-q_LF_K)+ torch.exp(-self.w_LFBD)*q_LF_K*(1-q_LS_K)\n",
    "#         BD_inner_2 = torch.exp(self.w_LSBD + self.w_LFBD)*q_LS_K*q_LF_K + (1-q_LS_K)*(1-q_LF_K) + torch.exp(self.w_LSBD)*q_LS_K*(1-q_LF_K)+ torch.exp(self.w_LFBD)*q_LF_K*(1-q_LS_K)\n",
    "#         p_z_BD = (q_BD_K*(-torch.log(1+torch.exp(-(self.w_eBD**2)/2 - self.w_0BD - self.w_aBD*a_BD)*BD_inner_1))\n",
    "#                  +(1-q_BD_K)*(-torch.log(1+torch.exp((self.w_eBD**2)/2 + self.w_0BD + self.w_aBD*a_BD)*BD_inner_2)))\n",
    "#         p_z_LS = (q_LS_K*(-torch.log(1+torch.exp(-(self.w_eLS**2)/2 - self.w_0LS - self.w_aLS*a_LS)))\n",
    "#                 + (1-q_LS_K)*(-torch.log(1+torch.exp((self.w_eLS**2)/2 + self.w_0LS + self.w_aLS*a_LS))))\n",
    "#         p_z_LF = (q_LF_K*(-torch.log(1+torch.exp(-(self.w_eLF**2)/2 - self.w_0LF - self.w_aLF*a_LF)))\n",
    "#                 + (1-q_LF_K)*(-torch.log(1+torch.exp((self.w_eLF**2)/2 + self.w_0LF + self.w_aLF*a_LF))))\n",
    "        p_g_1 = q_LS_K*(1-q_LF_K) + q_LF_K*(1-q_LS_K)\n",
    "        p_g_0 = q_LS_K*q_LF_K + (1-q_LS_K)*(1-q_LF_K)    \n",
    "        E_log_p_z_BD = q_BD_K*(-torch.log(1+torch.exp(-self.w_0BD - self.w_aBD*a_BD - (self.w_eBD**2)/2)*(torch.exp(-self.w_gBD)*p_g_1 + p_g_0))) + (1-q_BD_K)*(-torch.log(1+torch.exp(-self.w_0BD - self.w_aBD*a_BD - (self.w_eBD**2)/2)*(torch.exp(self.w_gBD)*p_g_1+p_g_0)))\n",
    "        E_log_p_z_LS = q_LS_K*(-torch.log(1+torch.exp(-self.w_0LS - self.w_aLS*a_LS + (self.w_eLS**2)/2))) + (1-q_LS_K)*(-torch.log(1+torch.exp(self.w_0LS + self.w_aLS*a_LS + (self.w_eLS**2)/2)))\n",
    "        E_log_p_z_LF = q_LF_K*(-torch.log(1+torch.exp(-self.w_0LF - self.w_aLF*a_LF + (self.w_eLF**2)/2))) + (1-q_LF_K)*(-torch.log(1+torch.exp(self.w_0LF + self.w_aLF*a_LF + (self.w_eLF**2)/2)))\n",
    "        E_log_p_z = torch.mean(E_log_p_z_BD + E_log_p_z_LS + E_log_p_z_LF)        \n",
    "\n",
    "#         E_g = q_LS_K + q_LF_K - 2*q_LS_K*q_LF_K\n",
    "#         r_gBD = self.w_0BD + self.w_gBD/self.t_gBD\n",
    "#         E_log_p_z_BD =q_BD_K*(self.f(self.w_0BD) + self.t_gBD*E_g*(self.f(r_gBD) - self.f(self.w_0BD)) \n",
    "#                      + self.t_aBD*a_BD*(self.f(r_aBD) - self.f(self.w_0BD))) + (1-q_BD_K)*(-self.w_gBD*E_g - self.w_aBD*a_BD - self.w_0BD)                                                           \n",
    "#         r_aLS = self.w_0LS + self.w_aLS/self.t_aLS  \n",
    "#         E_log_p_z_LS = q_LS_K*(self.f(self.w_0LS) + self.t_aLS*a_LS*(self.f(r_aLS)-self.f(self.w_0LS))) + (1-q_LS_K)*(-self.w_aLS*a_LS - self.w_0LS)\n",
    "#         r_aLF = self.w_0LF + self.w_aLF/self.t_aLF                                                      \n",
    "#         E_log_p_z_LF = q_LF_K*(self.f(self.w_0LF) + self.t_aLF*a_LF*(self.f(r_aLF)-self.f(self.w_0LF))) + (1-q_LF_K)*(-self.w_aLF*a_LF - self.w_0LF)                                               \n",
    "#         E_log_p_z = torch.mean(E_log_p_z_BD + E_log_p_z_LS + E_log_p_z_LF)\n",
    "\n",
    "        E_log_p_xz = (- torch.log(x) - torch.log(torch.abs(torch.sqrt(self.w_ex))) \n",
    "                   - (torch.log(x)**2)/(2*(self.w_ex))\n",
    "                   + (torch.log(x)*(self.w_BDx*q_BD_K + self.w_LSx*q_LS_K + self.w_LFx*q_LF_K + self.w_0x))/(self.w_ex)\n",
    "                   - ((self.w_BDx**2)*q_BD_K + (self.w_LSx**2)*q_LS_K + (self.w_LFx**2)*q_LF_K + self.w_ex**2 + self.w_0x**2)/(2*(self.w_ex))\n",
    "                   - (self.w_BDx*self.w_LSx*q_BD_K*q_LS_K + self.w_BDx*self.w_LFx*q_BD_K*q_LF_K + self.w_LSx*self.w_LFx*q_LS_K*q_LF_K)/(self.w_ex)\n",
    "                   - (self.w_BDx*self.w_0x*q_BD_K + self.w_LSx*self.w_0x*q_LS_K + self.w_LFx*self.w_0x*q_LF_K)/(self.w_ex))\n",
    "#         E_log_p_xz = -torch.log(x) - torch.log(torch.abs(self.w_ex)) - ((torch.log(x))**2 + self.w_0x**2)/(2*self.w_ex**2) - ((self.w_BDx**2)*q_BD_K + (self.w_LSx**2)*q_LS_K + (self.w_LFx**2)*q_LF_K)/(2*self.w_ex**2) - (self.w_0x*(self.w_BDx*q_BD_K + self.w_LSx*q_LS_K + self.w_LFx*q_LF_K))/(self.w_ex**2) - (self.w_BDx*self.w_LSx*q_BD_K*q_LS_K + self.w_BDx*self.w_LFx*q_BD_K*q_LF_K + self.w_LFx*self.w_LSx*q_LF_K*q_LS_K)/(self.w_ex**2) + (torch.log(x)*(self.w_0x + self.w_BDx*q_BD_K + self.w_LSx*q_LS_K + self.w_LFx*q_LF_K))/(self.w_ex**2) \n",
    "        return q_LS_mu, q_LS_var, q_LS_0, q_LS_K, self.log_det_j_LS, q_LF_mu, q_LF_var, q_LF_0, q_LF_K, self.log_det_j_LF, q_BD_mu, q_BD_var, q_BD_0, q_BD_K, self.log_det_j_BD, E_log_p_xz, E_log_p_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphicalVINF(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(GraphicalVINF, self).__init__()\n",
    "\n",
    "        # extract model settings from args\n",
    "        self.batch_size = args.batch_size\n",
    "        self.is_cuda = args.cuda\n",
    "        self.log_det_j_LS = 0.\n",
    "        self.log_det_j_LF = 0.\n",
    "        self.log_det_j_BD = 0.\n",
    "        self.num_pseudos = args.num_pseudos # for initialising pseudoinputs\n",
    "        \n",
    "        flowLS = Planar # For normalizing flow\n",
    "        flowLF = Planar\n",
    "        flowBD = Planar\n",
    "        self.num_flows = args.num_flows\n",
    "        self.m = nn.Sigmoid()\n",
    "        \n",
    "        # Normalizing flow layers\n",
    "        for k in range(self.num_flows):\n",
    "            flow_k_LS = flowLS()\n",
    "            self.add_module('flow_LS_' + str(k), flow_k_LS)\n",
    "        for k in range(self.num_flows):\n",
    "            flow_k_LF = flowLF()\n",
    "            self.add_module('flow_LF_' + str(k), flow_k_LF)\n",
    "        for k in range(self.num_flows):\n",
    "            flow_k_BD = flowBD()\n",
    "            self.add_module('flow_BD_' + str(k), flow_k_BD)\n",
    "        \n",
    "        \n",
    "        # Paramters in normalizing flows\n",
    "        self.mu = nn.Sequential(nn.Linear(self.batch_size, self.batch_size),\n",
    "                                nn.Hardtanh(min_val=-2, max_val=2))\n",
    "        self.var = nn.Sequential(\n",
    "            nn.Linear(self.batch_size, self.batch_size),\n",
    "            nn.Softplus(),\n",
    "            nn.Hardtanh(min_val=1, max_val=5))\n",
    "        self.amor_u = nn.Sequential(nn.Linear(self.batch_size, self.num_flows * self.batch_size),\n",
    "                                    nn.Hardtanh(min_val=-1, max_val=1))\n",
    "        self.amor_w = nn.Sequential(nn.Linear(self.batch_size, self.num_flows * self.batch_size),\n",
    "                                    nn.Hardtanh(min_val=-1, max_val=1))\n",
    "        self.amor_b = nn.Sequential(nn.Linear(self.batch_size, self.num_flows),\n",
    "                                    nn.Hardtanh(min_val=-1, max_val=1))\n",
    "\n",
    "\n",
    "        \n",
    "        self.begin = nn.Sequential(\n",
    "            nn.Linear(self.batch_size,self.batch_size),\n",
    "            nn.Sigmoid())\n",
    "        #nn.Softplus() = log(1+exp(x))\n",
    "        #Amortized flow parameters\n",
    "\n",
    "\n",
    "        # Parameters Setup\n",
    "        self.w_0LS = nn.Parameter(torch.rand(1, requires_grad=True, dtype=torch.float))\n",
    "        self.w_0LF = nn.Parameter(torch.rand(1, requires_grad=True, dtype=torch.float))\n",
    "        self.w_0BD = nn.Parameter(torch.rand(1, requires_grad=True, dtype=torch.float))\n",
    "        self.w_eLS = nn.Parameter(torch.rand(1, requires_grad=True, dtype=torch.float))\n",
    "        self.w_eLF = nn.Parameter(torch.rand(1, requires_grad=True, dtype=torch.float))\n",
    "        self.w_eBD = nn.Parameter(torch.rand(1, requires_grad=True, dtype=torch.float))\n",
    "        self.w_ex  = nn.Parameter(torch.rand(1, requires_grad=True, dtype=torch.float))\n",
    "        self.w_0x  = nn.Parameter(torch.rand(1, requires_grad=True, dtype=torch.float))\n",
    "        self.var_x = nn.Parameter(torch.rand(1, requires_grad=True, dtype=torch.float))\n",
    "        self.w_LSx = nn.Parameter(torch.rand(1, requires_grad=True, dtype=torch.float)) \n",
    "        self.w_LFx = nn.Parameter(torch.rand(1, requires_grad=True, dtype=torch.float)) \n",
    "        self.w_BDx = nn.Parameter(torch.rand(1, requires_grad=True, dtype=torch.float)) \n",
    "        self.w_LSBD = nn.Parameter(torch.rand(1, requires_grad=True, dtype=torch.float)) \n",
    "        self.w_LFBD = nn.Parameter(torch.rand(1, requires_grad=True, dtype=torch.float)) \n",
    "        self.w_aBD  = nn.Parameter(torch.rand(1, requires_grad=True, dtype=torch.float))/2\n",
    "        self.w_aLS  = nn.Parameter(torch.rand(1, requires_grad=True, dtype=torch.float))/2\n",
    "        self.w_aLF  = nn.Parameter(torch.rand(1, requires_grad=True, dtype=torch.float))/2\n",
    "        \n",
    "        if args.cuda:\n",
    "            self.FloatTensor = torch.cuda.FloatTensor\n",
    "        else:\n",
    "            self.FloatTensor = torch.FloatTensor\n",
    "      \n",
    "    def init_pseudoinputs(self, pseudo_inputs):\n",
    "        \"\"\"\n",
    "        Adds and initialises additional layer for pseudoinput generation\n",
    "        pseudo_inputs: either random training data or None\n",
    "        \"\"\"\n",
    "        self.pseudo_inputs = pseudo_inputs\n",
    "        \n",
    "        if pseudo_inputs is None:\n",
    "            # initialise dummy inputs\n",
    "            if self.is_cuda:\n",
    "                self.dummy_inputs = torch.eye(self.num_pseudos).cuda()\n",
    "            else:\n",
    "                self.dummy_inputs = torch.eye(self.num_pseudos)\n",
    "            self.dummy_inputs.requires_grad = False\n",
    "            # initialise layers for learning pseudoinputs\n",
    "            self.pseudo_layer = nn.Linear(self.num_pseudos, self.batch_size, bias=False)\n",
    "            #default in experiment parser\n",
    "            self.pseudo_nonlin = nn.Hardtanh(min_val=0.0, max_val=1.0)\n",
    "        else:        \n",
    "            if self.is_cuda:\n",
    "                self.pseudo_inputs = self.pseudo_inputs.cuda()     \n",
    "            self.pseudo_inputs.requires_grad = False\n",
    "\n",
    "            \n",
    "    def log_vamp_zk(self, zk):\n",
    "        \"\"\"\n",
    "        Calculates log p(z_k) under VampPrior\n",
    "        \"\"\"\n",
    "        # generate pseudoinputs from diagonal tensor\n",
    "        if self.pseudo_inputs is None:\n",
    "            pseudo_x = self.pseudo_nonlin(self.pseudo_layer(self.dummy_inputs))\n",
    "        else:\n",
    "            pseudo_x = self.pseudo_inputs\n",
    "            \n",
    "        # calculate VampPrior\n",
    "        vamp_mu, vamp_logvar, _, _, _ = self.amortize(pseudo_x)\n",
    "        \n",
    "        # expand\n",
    "        zk_expanded = zk.unsqueeze(1) # (batch_size, 1,          z_size)\n",
    "        mus = vamp_mu.unsqueeze(0)    # (1,         num_pseudos, z_size)\n",
    "        logvars = vamp_logvar.unsqueeze(0)    # (1, num_pseudos, z_size)\n",
    "        \n",
    "        # calculate log p(z_k)\n",
    "        log_per_pseudo = log_normal_dist(zk_expanded, mus, logvars, dim=1) - math.log(self.num_pseudos)\n",
    "        # (batch_size,num_pseudos)\n",
    "        log_total = torch.logsumexp(log_per_pseudo, 0)\n",
    "        # (batch_size)\n",
    "        return log_total\n",
    "    \n",
    "    def reparameterize(self, mu, var):\n",
    "        \"\"\"\n",
    "        Samples z from a multivariate Gaussian with diagonal covariance matrix using the\n",
    "         reparameterization trick.\n",
    "        \"\"\"\n",
    "        std = var.sqrt()\n",
    "        eps = torch.randn_like(std)\n",
    "        z = eps * std + mu\n",
    "        return self.m(z)\n",
    "\n",
    "    def amortize(self, a):\n",
    "        '''\n",
    "        Calculate the parameters obtained from the existing models for the base \n",
    "        distribution of normalizing flows\n",
    "        \n",
    "        '''\n",
    "        mu = self.mu(a)\n",
    "        var = self.var(a)\n",
    "        u = self.amor_u(a).view(-1, self.num_flows, self.batch_size, 1)\n",
    "        w = self.amor_w(a).view(-1, self.num_flows, 1, self.batch_size)\n",
    "        b = self.amor_b(a).view(-1, self.num_flows, 1, 1)\n",
    "        return mu, var, u, w, b\n",
    "\n",
    "    def forward(self, a_LS, a_LF, a_BD, x):      \n",
    "        if self.is_cuda:\n",
    "            self.log_det_j_LS = torch.zeros([x.shape[0]]).cuda()\n",
    "            self.log_det_j_LF = torch.zeros([x.shape[0]]).cuda()\n",
    "            self.log_det_j_BD = torch.zeros([x.shape[0]]).cuda()\n",
    "        else:\n",
    "            self.log_det_j_LS = torch.zeros([x.shape[0]])\n",
    "            self.log_det_j_LF = torch.zeros([x.shape[0]])\n",
    "            self.log_det_j_BD = torch.zeros([x.shape[0]])\n",
    "\n",
    "        a_LS_1 = self.w_aLS*a_LS + self.w_0LS\n",
    "        a_LF_1 = self.w_aLF*a_LF + self.w_0LF\n",
    "        \n",
    "#         a_LS_1[a_LS_1 > 1] = 1\n",
    "#         a_LF_1[a_LF_1 > 1] = 1\n",
    "\n",
    "        # mean and variance of z\n",
    "        z_LF_mu, y_LF_var, u_LF, w_LF, b_LF = self.amortize(a_LF_1)\n",
    "        z_LF_var = y_LF_var + self.w_eLF**2\n",
    "        z_LF = self.reparameterize(z_LF_mu, z_LF_var)\n",
    "        z_LF = [z_LF]\n",
    "        \n",
    "        z_LS_mu, y_LS_var, u_LS, w_LS, b_LS = self.amortize(a_LS_1)\n",
    "        z_LS_var = y_LS_var + self.w_eLS**2    \n",
    "        z_LS = self.reparameterize(z_LS_mu, z_LS_var)\n",
    "        z_LS = [z_LS]  \n",
    "        \n",
    "        for k in range(self.num_flows):\n",
    "            flow_k_LS = getattr(self, 'flow_LS_' + str(k))\n",
    "            z_LSk, log_det_jacobian_LS = flow_k_LS(z_LS[k], u_LS[:, k, :, :], w_LS[:, k, :, :], b_LS[:, k, :, :])\n",
    "            z_LS.append(z_LSk)\n",
    "            self.log_det_j_LS = self.log_det_j_LS + log_det_jacobian_LS  \n",
    "            \n",
    "        for k in range(self.num_flows):\n",
    "            flow_k_LF = getattr(self, 'flow_LF_' + str(k))\n",
    "            z_LFk, log_det_jacobian_LF = flow_k_LF(z_LF[k], u_LF[:, k, :, :], w_LF[:, k, :, :], b_LF[:, k, :, :])\n",
    "            z_LF.append(z_LFk)\n",
    "            self.log_det_j_LF = self.log_det_j_LF + log_det_jacobian_LF\n",
    "\n",
    "        \n",
    "\n",
    "        z_LS_0 = z_LS[0]\n",
    "        z_LF_0 = z_LF[0]\n",
    "        z_LS_K = self.m(z_LS[-1])\n",
    "        z_LF_K = self.m(z_LF[-1])\n",
    "        \n",
    "        a_BD_1 = self.w_LSBD*z_LS_K.view(-1,self.batch_size) + self.w_LFBD*z_LF_K.view(-1,self.batch_size) + self.w_aBD*a_BD + self.w_0BD\n",
    "#         a_BD_1[a_BD_1 > 1] = 1\n",
    "        z_BD_mu, y_BD_var, u_BD, w_BD, b_BD = self.amortize(a_BD_1)\n",
    "        z_BD_var = y_BD_var + self.w_eBD**2\n",
    "        \n",
    "        \n",
    "        z_BD = self.reparameterize(z_BD_mu, z_BD_var)\n",
    "        z_BD = [z_BD]\n",
    "        \n",
    "        for k in range(self.num_flows):\n",
    "            flow_k_BD = getattr(self, 'flow_BD_' + str(k))\n",
    "            z_BDk, log_det_jacobian_BD = flow_k_BD(z_BD[k], u_BD[:, k, :, :], w_BD[:, k, :, :], b_BD[:, k, :, :])\n",
    "            z_BD.append(z_BDk)\n",
    "            self.log_det_j_BD = self.log_det_j_BD + log_det_jacobian_BD\n",
    "            \n",
    "        z_BD_0 = z_BD[0]\n",
    "        z_BD_K = self.m(z_BD[-1])\n",
    "        \n",
    "\n",
    "        BD_inner_1 = torch.exp(- self.w_LSBD - self.w_LFBD)*z_LS_K*z_LF_K + (1-z_LS_K)*(1-z_LF_K) + torch.exp(-self.w_LSBD)*z_LS_K*(1-z_LF_K)+ torch.exp(-self.w_LFBD)*z_LF_K*(1-z_LS_K)\n",
    "        BD_inner_2 = torch.exp(self.w_LSBD + self.w_LFBD)*z_LS_K*z_LF_K + (1-z_LS_K)*(1-z_LF_K) + torch.exp(self.w_LSBD)*z_LS_K*(1-z_LF_K)+ torch.exp(self.w_LFBD)*z_LF_K*(1-z_LS_K)\n",
    "        p_z_BD = (z_BD_K*(-torch.log(1+torch.exp(-(self.w_eBD**2)/2 - self.w_0BD - self.w_aBD*a_BD)*BD_inner_1))\n",
    "                 +(1-z_BD_K)*(-torch.log(1+torch.exp((self.w_eBD**2)/2 + self.w_0BD + self.w_aBD*a_BD)*BD_inner_2)))\n",
    "        p_z_LS = (z_LS_K*(-torch.log(1+torch.exp(-(self.w_eLS**2)/2 - self.w_0LS - self.w_aLS*a_LS)))\n",
    "                + (1-z_LS_K)*(-torch.log(1+torch.exp((self.w_eLS**2)/2 + self.w_0LS + self.w_aLS*a_LS))))\n",
    "        p_z_LF = (z_LF_K*(-torch.log(1+torch.exp(-(self.w_eLF**2)/2 - self.w_0LF - self.w_aLF*a_LF)))\n",
    "                + (1-z_LF_K)*(-torch.log(1+torch.exp((self.w_eLF**2)/2 + self.w_0LF + self.w_aLF*a_LF))))\n",
    "        \n",
    "        logp_zk = torch.mean(p_z_BD + p_z_LS + p_z_LF)\n",
    "    \n",
    "        log_p_xz = (- torch.log(x) - torch.log(torch.abs(torch.sqrt(self.w_ex))) \n",
    "                   - (torch.log(x)**2)/(2*(self.w_ex**2))\n",
    "                   + (torch.log(x)*(self.w_BDx*z_BD_K + self.w_LSx*z_LS_K + self.w_LFx*z_LF_K + self.w_0x))/(self.w_ex**2)\n",
    "                   - ((self.w_BDx**2)*z_BD_K + (self.w_LSx**2)*z_LS_K + (self.w_LFx**2)*z_LF_K + self.w_ex**2 + self.w_0x**2)/(2*(self.w_ex**2))\n",
    "                   - (self.w_BDx*self.w_LSx*z_BD_K*z_LS_K + self.w_BDx*self.w_LFx*z_BD_K*z_LF_K + self.w_LSx*self.w_LFx*z_LS_K*z_LF_K)/(self.w_ex**2)\n",
    "                   - (self.w_BDx*self.w_0x*z_BD_K + self.w_LSx*self.w_0x*z_LS_K + self.w_LFx*self.w_0x*z_LF_K)/(self.w_ex)**2)\n",
    "                    \n",
    "\n",
    "        return z_LS_mu, z_LS_var, z_LS_0, z_LS_K, self.log_det_j_LS, z_LF_mu, z_LF_var, z_LF_0, z_LF_K, self.log_det_j_LF,z_BD_mu, z_BD_var, z_BD_0, z_BD_K, self.log_det_j_BD,log_p_xz,logp_zk\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnum = np.floor(total_num/args.batch_size)\n",
    "rand_idx = np.array(random.sample(range(int(bnum)*args.batch_size),int(bnum)*args.batch_size))\n",
    "IND = torch.from_numpy(rand_idx[0:int(bnum*args.batch_size)])\n",
    "IND = IND.view(-1, args.batch_size)\n",
    "IND = IND.numpy()\n",
    "\n",
    "aLS = np.array(PLS).reshape([total_num])\n",
    "aLS = aLS[rand_idx[0:int(bnum*args.batch_size)]]\n",
    "aLS = torch.from_numpy(np.array(aLS)).float()\n",
    "aLS = aLS.view(-1,args.batch_size)\n",
    "\n",
    "aLF = np.array(PLF).reshape([total_num])\n",
    "aLF = aLF[rand_idx[0:int(bnum*args.batch_size)]]\n",
    "aLF = torch.from_numpy(np.array(aLF)).float()\n",
    "aLF = PLF.view(-1,args.batch_size)\n",
    "\n",
    "aBD = np.array(PBD).reshape([total_num])\n",
    "aBD = aBD[rand_idx[0:int(bnum*args.batch_size)]]\n",
    "aBD = torch.from_numpy(np.array(aBD)).float()\n",
    "aBD = aBD.view(-1,args.batch_size)\n",
    "\n",
    "obs = np.array(OBS).reshape([total_num])\n",
    "obs = obs[rand_idx[0:int(bnum*args.batch_size)]]\n",
    "obs = torch.from_numpy(np.array(obs)).float()\n",
    "obs = obs.view(-1,args.batch_size)\n",
    "\n",
    "LS = aLS.detach()\n",
    "LF = aLF.detach()\n",
    "BD = aBD.detach()\n",
    "OB = obs.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_PLS = torch.tensor(PLS.view(total_num).numpy().tolist().copy()).view(-1,1)\n",
    "test_PLF = torch.tensor(PLF.view(total_num).numpy().tolist().copy()).view(-1,1)\n",
    "test_PBD = torch.tensor(PBD.view(total_num).numpy().tolist().copy()).view(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('\\nMODEL SETTINGS: \\n', args, '\\n')\n",
    "# print(\"Random Seed: \", args.manual_seed)\n",
    "\n",
    "dataset = MyDataset(LS, LF, BD, OB)\n",
    "MyDataLoader = DataLoader(dataset=dataset,shuffle=True)\n",
    "model = GraphicalVINF(args)\n",
    "args.vampprior = False\n",
    "# if args.vampprior:\n",
    "#     load = torch.utils.data.DataLoader(MyDataLoader.dataset, batch_size=args.num_pseudos, shuffle=True)\n",
    "#     pseudo_inputs = None\n",
    "#     model.init_pseudoinputs(pseudo_inputs)\n",
    "if args.cuda:\n",
    "    print(\"Model on GPU\")\n",
    "    model.cuda()\n",
    "# print(model)\n",
    "opt = optim.RMSprop(model.parameters(), lr=args.learning_rate, momentum=0.9)\n",
    "loss = []\n",
    "t_loss = 1e-6\n",
    "epoch = 0\n",
    "t = time.time()\n",
    "\n",
    "TLoss = []\n",
    "epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logp_zk:  tensor(-2.1200, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-4.3611, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-0.6979], grad_fn=<DivBackward0>)\n",
      "Epoch:   1 [( 0%)]  \tLoss:   -1.731237\tkl:   -3.959098\n",
      "logp_zk:  tensor(-2.1260, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-4.2169, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-0.6754], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.1073, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-4.3085, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-0.6651], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.1057, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-4.2709, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-0.6681], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.1112, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-4.1989, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-0.6588], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.1019, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-4.3656, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-0.6879], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.1343, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-4.2428, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-0.6635], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.1289, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-4.2268, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-0.6851], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.1263, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-4.3501, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-0.6889], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.1413, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-4.1537, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-0.6613], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.1396, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-4.2333, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-0.6904], grad_fn=<DivBackward0>)\n",
      "Epoch:   1 [( 2%)]  \tLoss:    3.310564\tkl:   -0.377990\n",
      "logp_zk:  tensor(-2.1296, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-4.1762, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-0.6966], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.1181, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-4.0853, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-0.7088], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.1173, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-4.0951, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-0.7188], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.1090, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-4.2595, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-0.7232], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.1378, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-4.2049, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-0.6844], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.1289, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-4.2296, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-0.6982], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.1344, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-4.0424, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-0.7202], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.1442, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-4.0051, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-0.7091], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.1487, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-4.0078, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-0.7156], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.1273, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-4.0204, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-0.7185], grad_fn=<DivBackward0>)\n",
      "Epoch:   1 [( 4%)]  \tLoss:    4.264056\tkl:   -0.681386\n",
      "logp_zk:  tensor(-2.1310, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-3.9753, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-0.7221], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.1434, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-3.9166, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-0.7356], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.1399, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-4.0126, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-0.7182], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.1575, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-4.0438, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-0.7369], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.1629, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-3.8651, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-0.7129], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.1608, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-3.7374, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-0.7225], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.1665, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-3.8304, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-0.7427], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.1573, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-3.9485, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-0.7419], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.1559, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-3.8750, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-0.7226], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.1564, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-3.8664, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-0.7743], grad_fn=<DivBackward0>)\n",
      "Epoch:   1 [( 6%)]  \tLoss:    7.186794\tkl:    1.382177\n",
      "logp_zk:  tensor(-2.1531, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-3.8937, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-0.7738], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.1670, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-3.9499, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-0.7229], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.1634, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-3.8254, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-0.7578], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.1608, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-3.7513, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-0.7658], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.1632, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-3.7963, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-0.7658], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.1641, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-3.8307, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-0.7655], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.1767, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-3.7778, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-0.7613], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.1763, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-3.9794, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-0.7579], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.1762, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-3.7073, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-0.7586], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.1869, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-3.9781, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-0.7429], grad_fn=<DivBackward0>)\n",
      "Epoch:   1 [( 8%)]  \tLoss:    7.408773\tkl:    1.444025\n",
      "logp_zk:  tensor(-2.1879, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-3.7839, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-0.7459], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.1945, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-3.6950, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-0.7622], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.1871, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-3.6717, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-0.7591], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.1916, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-3.6600, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-0.7669], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.1923, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-3.6515, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-0.7744], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.1920, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-3.8291, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-0.7760], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.2012, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-3.6184, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-0.7670], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.2014, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-3.7114, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-0.7707], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.2055, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-3.7207, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-0.7725], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.2086, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-3.7514, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-0.7751], grad_fn=<DivBackward0>)\n",
      "Epoch:   1 [(10%)]  \tLoss:    7.179488\tkl:    1.433435\n",
      "logp_zk:  tensor(-2.2058, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-3.5820, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-0.7761], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.2184, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-3.5853, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-0.7884], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.2144, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-3.6687, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-0.8030], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.2190, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-3.5933, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-0.7513], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.2233, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-3.6219, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-0.7768], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.2195, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-3.5551, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-0.7914], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.2248, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-3.6185, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-0.7881], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.2211, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-3.5950, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-0.8049], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.2252, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-3.5443, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-0.8286], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.2341, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-3.5036, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-0.7814], grad_fn=<DivBackward0>)\n",
      "Epoch:   1 [(12%)]  \tLoss:    6.869779\tkl:    1.452764\n",
      "logp_zk:  tensor(-2.2253, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-3.4280, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-0.7971], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.2365, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-3.3550, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-0.8293], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.2426, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-3.5172, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-0.8111], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.2419, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-3.5843, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-0.7947], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.2451, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-3.6740, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-0.8218], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.2506, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-3.3917, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-0.7793], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.2511, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-3.4361, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-0.8117], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.2467, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-3.4726, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-0.8320], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.2582, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-3.3431, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-0.8473], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.2552, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-3.4307, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-0.8469], grad_fn=<DivBackward0>)\n",
      "Epoch:   1 [(14%)]  \tLoss:    6.906893\tkl:    1.408338\n",
      "logp_zk:  tensor(-2.2520, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-3.3542, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-0.8402], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.2630, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-3.3823, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-0.8530], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.2652, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-3.3475, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-0.8264], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.2741, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-3.4616, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-0.8386], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.2679, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-3.5123, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-0.8697], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.2796, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-3.3297, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-0.8475], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.2792, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-3.2882, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-0.8635], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.2698, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-3.1913, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-0.8610], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.2764, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-3.4611, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-0.8747], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.2763, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-3.3162, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-0.8683], grad_fn=<DivBackward0>)\n",
      "Epoch:   1 [(16%)]  \tLoss:    6.647218\tkl:    1.407965\n",
      "logp_zk:  tensor(-2.2835, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-3.3940, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-0.9028], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.2805, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-3.3338, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-0.8894], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.2962, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-3.3620, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-0.8637], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.2823, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-3.3909, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-0.8864], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.2924, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-3.3553, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-0.8705], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.2984, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-3.2643, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-0.8974], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.2964, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-3.3794, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-0.8907], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.2985, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-3.3173, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-0.9117], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.3042, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-3.3022, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-0.8906], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.3093, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-3.2107, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-0.8785], grad_fn=<DivBackward0>)\n",
      "Epoch:   1 [(18%)]  \tLoss:    6.611777\tkl:    1.430754\n",
      "logp_zk:  tensor(-2.3133, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-3.3268, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-0.8844], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.3144, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-3.2953, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-0.8847], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.3215, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-3.2185, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-0.8674], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.3164, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-3.3159, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-0.9005], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.3203, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-3.3117, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-0.9080], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.3207, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-3.1456, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-0.9225], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.3241, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-3.1859, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-0.9360], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.3256, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-3.0695, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-0.9481], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.3353, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-3.2644, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-0.8942], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.3371, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-3.0012, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-0.9210], grad_fn=<DivBackward0>)\n",
      "Epoch:   1 [(20%)]  \tLoss:    6.357184\tkl:    1.416068\n",
      "logp_zk:  tensor(-2.3353, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-3.2524, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-0.9445], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.3420, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-3.0807, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-0.9297], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.3370, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-3.1815, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-0.9566], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.3456, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-3.2246, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-0.9170], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.3410, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-3.0924, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-0.9517], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.3499, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-3.0653, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-0.9230], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.3470, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-3.1385, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-0.9770], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.3644, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-3.0050, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-0.9489], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.3506, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-3.1974, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-0.9537], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.3634, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-3.0078, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-0.9323], grad_fn=<DivBackward0>)\n",
      "Epoch:   1 [(22%)]  \tLoss:    6.424765\tkl:    1.431171\n",
      "logp_zk:  tensor(-2.3624, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-2.9999, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-0.9564], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.3630, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-3.0924, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-0.9595], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.3642, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-2.9860, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-0.9837], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.3705, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-3.0035, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-0.9560], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.3729, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-3.0133, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-0.9872], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.3675, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-3.0219, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-0.9664], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.3772, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-3.0532, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-0.9700], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.3748, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-2.9543, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-1.0005], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.3836, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-3.0704, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-0.9881], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.3748, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-2.8940, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-1.0263], grad_fn=<DivBackward0>)\n",
      "Epoch:   1 [(24%)]  \tLoss:    6.267954\tkl:    1.348547\n",
      "logp_zk:  tensor(-2.3876, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-2.8781, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-1.0020], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.3864, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-3.0113, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-1.0074], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.3938, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-2.9348, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-1.0115], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.3862, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-2.9068, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-1.0246], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.3870, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-3.0344, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-1.0353], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.4001, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-2.8904, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-1.0098], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.3970, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-3.0028, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-1.0167], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.4107, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-2.9524, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-1.0331], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.4002, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-3.0071, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-1.0458], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.4105, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-2.9068, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-1.0199], grad_fn=<DivBackward0>)\n",
      "Epoch:   1 [(26%)]  \tLoss:    6.350548\tkl:    1.390586\n",
      "logp_zk:  tensor(-2.4062, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-2.8862, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-1.0708], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.4055, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-2.9490, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-1.0741], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.4193, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-2.9186, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-1.0592], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.4192, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-2.9460, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-1.0620], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.4197, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-2.8303, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-1.0844], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.4252, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-2.8342, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-1.0504], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.4230, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-2.8441, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-1.0848], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.4320, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-2.8176, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-1.0593], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.4217, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-2.7911, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-1.1091], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.4271, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-2.8474, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-1.1040], grad_fn=<DivBackward0>)\n",
      "Epoch:   1 [(28%)]  \tLoss:    6.149563\tkl:    1.323139\n",
      "logp_zk:  tensor(-2.4350, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-2.8230, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-1.0958], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.4350, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-2.8038, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-1.0806], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.4377, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-2.9424, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-1.0543], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.4324, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-2.8002, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-1.1456], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.4410, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-2.7622, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-1.1036], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.4407, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-2.7949, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-1.0862], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.4490, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-2.7238, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-1.1182], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.4517, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-2.7142, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-1.1434], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.4468, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-2.7625, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-1.1091], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.4601, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-2.8129, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-1.1237], grad_fn=<DivBackward0>)\n",
      "Epoch:   1 [(30%)]  \tLoss:    6.188238\tkl:    1.336366\n",
      "logp_zk:  tensor(-2.4545, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-2.8149, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-1.1236], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.4591, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-2.6862, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-1.1265], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.4653, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-2.7518, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-1.1348], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.4565, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-2.7518, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-1.1501], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.4698, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-2.6613, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-1.1618], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.4668, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-2.7169, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-1.1637], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.4711, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-2.6074, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-1.1554], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.4789, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-2.6461, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-1.1673], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.4796, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-2.6328, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-1.1528], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.4696, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-2.6692, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-1.1620], grad_fn=<DivBackward0>)\n",
      "Epoch:   1 [(32%)]  \tLoss:    5.916598\tkl:    1.307652\n",
      "logp_zk:  tensor(-2.4735, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-2.6918, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-1.2053], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.4733, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-2.7088, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-1.1762], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.4886, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-2.6190, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-1.2021], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.4872, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-2.7099, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-1.1708], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.4843, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-2.6404, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-1.2083], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.4832, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-2.7735, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-1.2202], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.4866, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-2.6348, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-1.2403], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.4949, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-2.5627, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-1.2192], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.4950, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-2.5325, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-1.2066], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.4991, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-2.7163, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-1.2078], grad_fn=<DivBackward0>)\n",
      "Epoch:   1 [(34%)]  \tLoss:    5.955161\tkl:    1.291233\n",
      "logp_zk:  tensor(-2.5012, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-2.6300, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-1.2136], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.5060, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-2.5034, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-1.2344], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.5081, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-2.5078, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-1.2222], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.5167, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-2.5099, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-1.2286], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.5134, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-2.5406, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-1.2318], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.5151, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-2.5635, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-1.2282], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.5137, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-2.5677, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-1.2721], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.5137, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-2.4254, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-1.2783], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.5174, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-2.4877, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-1.2785], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.5257, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-2.4453, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-1.2860], grad_fn=<DivBackward0>)\n",
      "Epoch:   1 [(36%)]  \tLoss:    5.658898\tkl:    1.239625\n",
      "logp_zk:  tensor(-2.5127, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-2.5960, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-1.2864], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.5309, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-2.5388, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-1.2674], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.5206, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-2.4953, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-1.3107], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.5279, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-2.5712, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-1.3328], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.5335, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-2.5293, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-1.2661], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.5274, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-2.4503, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-1.2850], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.5317, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-2.4934, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-1.3183], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.5369, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-2.4621, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-1.3285], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.5427, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-2.4609, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-1.3201], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.5427, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-2.5268, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-1.3433], grad_fn=<DivBackward0>)\n",
      "Epoch:   1 [(38%)]  \tLoss:    5.721634\tkl:    1.199436\n",
      "logp_zk:  tensor(-2.5505, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-2.3866, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-1.3110], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.5472, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-2.4107, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-1.3312], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.5436, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-2.4433, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-1.3547], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.5523, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-2.4061, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-1.3259], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.5498, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-2.4538, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-1.3703], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.5469, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-2.4425, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-1.3457], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.5483, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-2.4725, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-1.3923], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.5620, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-2.4403, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-1.3705], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.5583, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-2.4089, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-1.3746], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.5682, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-2.4293, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-1.3780], grad_fn=<DivBackward0>)\n",
      "Epoch:   1 [(40%)]  \tLoss:    5.593607\tkl:    1.190263\n",
      "logp_zk:  tensor(-2.5591, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-2.4041, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-1.3839], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.5647, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-2.3796, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-1.3911], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.5614, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-2.3484, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-1.3965], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.5638, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-2.3380, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-1.4155], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.5708, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-2.3800, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-1.4649], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.5739, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-2.4647, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-1.4179], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.5757, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-2.2975, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-1.4344], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.5755, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-2.5299, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-1.3944], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.5848, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-2.3402, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-1.3992], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.5832, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-2.3400, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-1.4277], grad_fn=<DivBackward0>)\n",
      "Epoch:   1 [(42%)]  \tLoss:    5.494382\tkl:    1.155496\n",
      "logp_zk:  tensor(-2.5872, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-2.4534, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-1.4835], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.5901, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-2.3083, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-1.4486], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.5841, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-2.3158, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-1.5026], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.5891, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-2.2901, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-1.4755], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.5955, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-2.2373, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-1.4842], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.5938, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-2.2324, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-1.4808], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.5947, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-2.3605, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-1.4859], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.6058, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-2.3841, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-1.4758], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.5989, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-2.3285, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-1.5008], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.5934, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-2.3874, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-1.5175], grad_fn=<DivBackward0>)\n",
      "Epoch:   1 [(44%)]  \tLoss:    5.544163\tkl:    1.075858\n",
      "logp_zk:  tensor(-2.6071, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-2.2933, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-1.5048], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.6056, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-2.2650, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-1.5042], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.6198, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-2.2172, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-1.5547], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.6121, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-2.2246, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-1.5432], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.6184, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-2.2093, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-1.5283], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.6181, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-2.1380, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-1.5414], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.6176, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-2.1989, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-1.5453], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.6218, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-2.1962, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-1.5703], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.6237, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-2.3286, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-1.5827], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.6278, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-2.1688, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-1.5626], grad_fn=<DivBackward0>)\n",
      "Epoch:   1 [(46%)]  \tLoss:    5.174295\tkl:    1.065165\n",
      "logp_zk:  tensor(-2.6198, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-2.1686, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-1.5622], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.6366, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-2.2218, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-1.5552], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.6286, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-2.1559, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-1.5762], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.6332, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-2.3555, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-1.5942], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.6313, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-2.1762, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-1.6038], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.6262, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-2.2722, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-1.5896], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.6359, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-2.1862, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-1.5834], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.6355, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-2.1776, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-1.6310], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.6470, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-2.1281, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-1.6175], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.6533, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-2.1573, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-1.6287], grad_fn=<DivBackward0>)\n",
      "Epoch:   1 [(48%)]  \tLoss:    5.175286\tkl:    1.024656\n",
      "logp_zk:  tensor(-2.6473, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-2.0820, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-1.6423], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.6380, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-2.0753, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-1.6529], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.6466, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-2.1076, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-1.6357], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.6436, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-2.0587, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-1.6772], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.6489, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-2.1337, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-1.7027], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.6557, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-2.0769, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-1.6806], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.6598, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-2.1334, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-1.6818], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.6598, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-2.0995, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-1.6866], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.6591, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-2.1251, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-1.7040], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.6662, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-2.0742, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-1.6772], grad_fn=<DivBackward0>)\n",
      "Epoch:   1 [(50%)]  \tLoss:    5.095958\tkl:    0.989041\n",
      "logp_zk:  tensor(-2.6654, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-2.1312, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-1.6973], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.6681, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-2.1225, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-1.6973], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.6678, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-2.0257, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-1.7367], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.6665, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-2.2601, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-1.7582], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.6712, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-2.0532, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-1.7347], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.6789, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-2.0790, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-1.7224], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.6722, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-2.1345, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-1.7189], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.6748, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-2.0180, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-1.7668], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.6775, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-2.1406, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-1.7335], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.6772, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-2.0239, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-1.7482], grad_fn=<DivBackward0>)\n",
      "Epoch:   1 [(52%)]  \tLoss:    4.937570\tkl:    0.928968\n",
      "logp_zk:  tensor(-2.6795, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-2.1466, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-1.8053], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.6845, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-2.0150, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-1.7616], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.6859, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-2.0490, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-1.7926], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.6823, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-2.0445, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-1.7829], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.6861, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-2.1200, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-1.7937], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.6913, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-2.0006, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-1.7948], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.6895, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-2.0251, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-1.8400], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.6913, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-2.0208, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-1.8077], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.6992, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.9186, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-1.8103], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.7030, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-2.0265, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-1.8248], grad_fn=<DivBackward0>)\n",
      "Epoch:   1 [(54%)]  \tLoss:    4.855312\tkl:    0.878170\n",
      "logp_zk:  tensor(-2.7034, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.8969, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-1.8226], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.7079, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.9512, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-1.8432], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.7105, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-2.0480, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-1.8367], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.7041, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-2.0046, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-1.8694], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.7168, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-2.0367, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-1.9009], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.7100, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.9141, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-1.8627], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.7168, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.9255, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-1.8899], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.7136, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.9311, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-1.9245], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.7206, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.9433, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-1.9447], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.7095, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.9981, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-1.9091], grad_fn=<DivBackward0>)\n",
      "Epoch:   1 [(56%)]  \tLoss:    4.731327\tkl:    0.800366\n",
      "logp_zk:  tensor(-2.7287, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.8645, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-1.8838], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.7180, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-2.0276, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-1.9575], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.7235, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.9652, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-1.9351], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.7211, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.8650, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-1.9026], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.7315, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.9504, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-1.9460], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.7305, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.8069, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-1.9441], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.7282, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.9805, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-1.9561], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.7312, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.9258, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-1.9852], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.7306, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.8954, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-1.9820], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.7339, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.8684, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-1.9798], grad_fn=<DivBackward0>)\n",
      "Epoch:   1 [(58%)]  \tLoss:    4.519995\tkl:    0.754194\n",
      "logp_zk:  tensor(-2.7383, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.8859, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-1.9302], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.7444, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.8508, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-2.0149], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.7439, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.8702, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-1.9777], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.7390, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.8627, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-2.0319], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.7522, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.8885, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-2.0059], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.7467, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.8615, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-2.0269], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.7469, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.8140, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-2.0393], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.7431, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.8489, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-2.0449], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.7488, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.8556, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-2.0469], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.7621, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.8204, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-2.0275], grad_fn=<DivBackward0>)\n",
      "Epoch:   1 [(60%)]  \tLoss:    4.576378\tkl:    0.734646\n",
      "logp_zk:  tensor(-2.7492, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.7693, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-2.0688], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.7550, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.8986, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-2.0863], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.7519, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.9032, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-2.0755], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.7589, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.7860, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-2.1089], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.7627, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.7987, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-2.1150], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.7672, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.7250, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-2.1341], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.7620, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.7877, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-2.1366], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.7684, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.7955, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-2.1203], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.7750, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.7821, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-2.1445], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.7708, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.7177, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-2.1634], grad_fn=<DivBackward0>)\n",
      "Epoch:   1 [(62%)]  \tLoss:    4.259810\tkl:    0.607374\n",
      "logp_zk:  tensor(-2.7698, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.7464, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-2.1472], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.7690, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.7989, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-2.1362], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.7726, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.8699, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-2.1878], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.7811, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.7009, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-2.1407], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.7763, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.8359, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-2.1951], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.7824, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.8716, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-2.1985], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.7775, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.8355, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-2.2073], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.7867, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.6122, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-2.2208], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.7857, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.7267, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-2.2523], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.7831, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.7093, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-2.2083], grad_fn=<DivBackward0>)\n",
      "Epoch:   1 [(64%)]  \tLoss:    4.248060\tkl:    0.574759\n",
      "logp_zk:  tensor(-2.7818, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.8394, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-2.2548], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.7939, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.7297, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-2.2729], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.7861, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.7667, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-2.2714], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.7863, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.6462, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-2.2863], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.7915, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.7067, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-2.2596], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.7914, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.7199, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-2.2895], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.8015, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.6911, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-2.2938], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.8027, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.7227, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-2.3114], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.7923, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.7451, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-2.3366], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.8040, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.6785, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-2.3185], grad_fn=<DivBackward0>)\n",
      "Epoch:   1 [(66%)]  \tLoss:    4.099304\tkl:    0.485487\n",
      "logp_zk:  tensor(-2.7937, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.7801, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-2.3365], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.8025, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.6696, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-2.3416], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.8014, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.6852, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-2.3927], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.8053, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.6728, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-2.3805], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.8073, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.6890, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-2.3443], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.8143, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.6356, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-2.3626], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.8139, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.6998, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-2.3928], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.8139, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.6991, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-2.3802], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.8131, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.7476, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-2.4135], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.8126, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.6634, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-2.3971], grad_fn=<DivBackward0>)\n",
      "Epoch:   1 [(68%)]  \tLoss:    4.051567\tkl:    0.415519\n",
      "logp_zk:  tensor(-2.8236, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.6013, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-2.4297], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.8235, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.5665, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-2.3921], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.8278, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.7015, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-2.4416], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.8372, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.6177, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-2.4566], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.8252, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.6403, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-2.4466], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.8308, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.5540, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-2.4818], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.8309, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.6710, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-2.4401], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.8334, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.6241, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-2.4910], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.8364, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.5906, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-2.4943], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.8390, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.5777, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-2.4937], grad_fn=<DivBackward0>)\n",
      "Epoch:   1 [(70%)]  \tLoss:    3.887324\tkl:    0.345308\n",
      "logp_zk:  tensor(-2.8379, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.5507, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-2.5033], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.8409, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.5637, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-2.5413], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.8420, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.6285, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-2.4885], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.8429, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.5397, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-2.5134], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.8324, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.5864, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-2.5683], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.8379, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.6335, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-2.5992], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.8482, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.5132, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-2.5385], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.8429, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.5456, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-2.6042], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.8511, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.5416, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-2.5788], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.8617, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.6242, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-2.5721], grad_fn=<DivBackward0>)\n",
      "Epoch:   1 [(72%)]  \tLoss:    3.911143\tkl:    0.289594\n",
      "logp_zk:  tensor(-2.8553, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.4589, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-2.6389], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.8477, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.6327, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-2.6202], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.8593, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.5158, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-2.6180], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.8529, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.5909, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-2.5881], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.8552, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.6169, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-2.6423], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.8539, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.5906, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-2.6337], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.8543, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.5287, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-2.6570], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.8560, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.5884, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-2.6392], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.8564, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.5254, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-2.6949], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.8696, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.5092, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-2.6802], grad_fn=<DivBackward0>)\n",
      "Epoch:   1 [(74%)]  \tLoss:    3.702094\tkl:    0.189487\n",
      "logp_zk:  tensor(-2.8662, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.4966, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-2.7006], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.8713, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.4361, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-2.7549], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.8692, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.4747, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-2.6851], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.8724, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.4467, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-2.7244], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.8714, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.5286, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-2.6692], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.8764, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.5668, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-2.7097], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.8677, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.5503, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-2.7569], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.8772, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.4750, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-2.7417], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.8796, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.4619, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-2.7882], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.8842, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.4383, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-2.7644], grad_fn=<DivBackward0>)\n",
      "Epoch:   1 [(76%)]  \tLoss:    3.566927\tkl:    0.119875\n",
      "logp_zk:  tensor(-2.8827, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.6403, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-2.7842], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.8821, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.4406, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-2.7931], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.8812, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.4700, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-2.8286], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.8882, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.4479, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-2.7648], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.8890, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.4356, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-2.8484], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.8865, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.4485, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-2.8532], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.8908, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.4664, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-2.8792], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.8912, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.4235, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-2.8900], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.8937, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.4507, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-2.8705], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.9004, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.4300, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-2.8785], grad_fn=<DivBackward0>)\n",
      "Epoch:   1 [(78%)]  \tLoss:    3.430265\tkl:    0.021942\n",
      "logp_zk:  tensor(-2.8899, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.4268, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-2.9722], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.9032, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.4064, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-2.8974], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.8983, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.4551, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-2.9654], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.8967, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.4070, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-2.9527], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.8966, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.4163, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-2.9428], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.9002, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.4046, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-2.9555], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.9121, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.2928, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-2.9312], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.9059, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.3775, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-2.9640], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.9157, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.3632, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-2.9806], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.9122, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.4331, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-3.0255], grad_fn=<DivBackward0>)\n",
      "Epoch:   1 [(80%)]  \tLoss:    3.335031\tkl:   -0.113332\n",
      "logp_zk:  tensor(-2.9111, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.4408, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-3.0211], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.9140, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.4293, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-3.0163], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.9188, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.3908, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-2.9843], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.9093, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.3979, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-3.0423], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.9250, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.2561, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-3.0759], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.9125, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.4237, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-3.0705], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.9087, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.4418, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-3.1072], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.9215, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.4173, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-3.0976], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.9246, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.3084, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-3.1112], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.9223, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.3354, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-3.1699], grad_fn=<DivBackward0>)\n",
      "Epoch:   1 [(82%)]  \tLoss:    3.106289\tkl:   -0.247645\n",
      "logp_zk:  tensor(-2.9297, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.2462, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-3.1576], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.9287, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.3515, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-3.1522], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.9241, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.3724, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-3.1743], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.9282, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.4191, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-3.1208], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.9270, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.2851, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-3.2084], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.9348, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.3886, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-3.1896], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.9305, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.4057, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-3.2042], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.9300, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.3552, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-3.2127], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.9366, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.3548, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-3.2277], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.9382, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.2853, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-3.2285], grad_fn=<DivBackward0>)\n",
      "Epoch:   1 [(84%)]  \tLoss:    2.954531\tkl:   -0.290271\n",
      "logp_zk:  tensor(-2.9401, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.4253, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-3.2023], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.9385, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.3368, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-3.2093], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.9417, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.2766, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-3.2011], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.9366, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.3695, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-3.2830], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.9325, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.4011, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-3.2502], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.9406, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.3311, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-3.2573], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.9414, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.4053, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-3.2744], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.9472, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.2907, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-3.3094], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.9465, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.3288, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-3.3659], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.9470, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.2875, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-3.3351], grad_fn=<DivBackward0>)\n",
      "Epoch:   1 [(86%)]  \tLoss:    2.847321\tkl:   -0.388112\n",
      "logp_zk:  tensor(-2.9545, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.2592, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-3.3690], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.9501, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.2495, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-3.4009], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.9633, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.2685, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-3.3330], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.9513, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.2385, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-3.3536], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.9599, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.2030, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-3.4122], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.9569, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.3139, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-3.4021], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.9581, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.3340, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-3.4564], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.9634, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.2021, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-3.4480], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.9615, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.2732, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-3.4846], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.9696, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.2735, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-3.4405], grad_fn=<DivBackward0>)\n",
      "Epoch:   1 [(88%)]  \tLoss:    2.804134\tkl:   -0.470853\n",
      "logp_zk:  tensor(-2.9632, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.2404, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-3.4811], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.9700, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.2381, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-3.5158], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.9685, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.1899, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-3.5117], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.9712, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.2767, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-3.4859], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.9654, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.2524, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-3.5364], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.9773, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.2768, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-3.5385], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.9759, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.2444, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-3.5267], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.9748, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.1930, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-3.5226], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.9760, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.1915, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-3.5885], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.9710, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.2831, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-3.5824], grad_fn=<DivBackward0>)\n",
      "Epoch:   1 [(90%)]  \tLoss:    2.674217\tkl:   -0.611362\n",
      "logp_zk:  tensor(-2.9865, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.1594, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-3.5978], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.9857, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.2806, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-3.6367], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.9710, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.3323, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-3.6260], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.9818, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.1991, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-3.6675], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.9848, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.1662, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-3.6741], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.9891, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.2306, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-3.6228], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.9792, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.2503, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-3.6736], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.9842, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.2188, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-3.7049], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.9852, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.1424, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-3.6838], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.9875, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.1887, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-3.7272], grad_fn=<DivBackward0>)\n",
      "Epoch:   1 [(92%)]  \tLoss:    2.450256\tkl:   -0.739637\n",
      "logp_zk:  tensor(-2.9934, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.2304, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-3.7139], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.9873, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.2315, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-3.6786], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.9955, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.2254, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-3.7673], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.9917, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.1846, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-3.7532], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.9901, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.2398, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-3.7998], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.9889, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.2201, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-3.7352], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.9948, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.1569, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-3.7885], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.9924, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.1523, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-3.8146], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.9979, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.1862, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-3.8233], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.9997, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.2072, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-3.8154], grad_fn=<DivBackward0>)\n",
      "Epoch:   1 [(94%)]  \tLoss:    2.363999\tkl:   -0.815679\n",
      "logp_zk:  tensor(-3.0076, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.1264, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-3.8230], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-3.0077, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.2097, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-3.8341], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-3.0042, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.1920, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-3.8432], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-2.9997, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.1806, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-3.8692], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-3.0019, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.2242, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-3.8954], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-3.0099, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.0894, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-3.9014], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-3.0046, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.1282, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-3.9056], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-3.0143, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.1409, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-3.9041], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-3.0174, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.0903, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-3.8861], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-3.0081, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.1164, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-3.9693], grad_fn=<DivBackward0>)\n",
      "Epoch:   1 [(96%)]  \tLoss:    2.153502\tkl:   -0.961182\n",
      "logp_zk:  tensor(-3.0069, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.1741, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-3.9380], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-3.0151, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.0932, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-3.9081], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-3.0124, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.1675, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-3.9585], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-3.0162, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.1102, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-3.9332], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-3.0222, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.1562, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-3.9745], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-3.0187, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.0670, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-4.0022], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-3.0175, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.1926, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-4.0553], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-3.0169, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.1446, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-4.0261], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-3.0223, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.0917, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-4.0288], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-3.0205, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.0836, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-4.0489], grad_fn=<DivBackward0>)\n",
      "Epoch:   1 [(98%)]  \tLoss:    2.025996\tkl:   -1.028483\n",
      "logp_zk:  tensor(-3.0237, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.0925, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-4.0452], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-3.0182, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.1008, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-4.0838], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-3.0277, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.0865, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-4.0220], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-3.0199, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.1892, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-4.1526], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-3.0251, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.0803, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-4.0998], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-3.0304, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.0873, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-4.1243], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-3.0301, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.0327, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-4.1107], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-3.0360, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.0354, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-4.1706], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-3.0334, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.1385, grad_fn=<MeanBackward0>)\n",
      "log_q_z0:  tensor([-4.1239], grad_fn=<DivBackward0>)\n",
      "logp_zk:  tensor(-3.0312, grad_fn=<MeanBackward0>)\n",
      "log_p_xz:  tensor(-1.1178, grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 500 is out of bounds for axis 0 with size 500",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-3d4aa2faeb3a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0mnew_q_LF_K\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mq_LF_K\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0mnew_q_BD_K\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mq_BD_K\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m             \u001b[0mtest_PLS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mIND\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_q_LS_K\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m             \u001b[0mtest_PLF\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mIND\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_q_LF_K\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0mtest_PBD\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mIND\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_q_BD_K\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 500 is out of bounds for axis 0 with size 500"
     ]
    }
   ],
   "source": [
    "\n",
    "# while epoch <= 10:\n",
    "\n",
    "for epoch in range(1, args.epochs + 1):\n",
    "    \n",
    "    \n",
    "    train_loss = np.zeros(len(MyDataLoader))\n",
    "    index = -1\n",
    "    i = 0\n",
    "    batch_size = args.batch_size\n",
    "    cuda = args.cuda\n",
    "\n",
    "    while t_loss > 0:\n",
    "        for aLS, aLF, aBD, obs in MyDataLoader: \n",
    "            index += 1\n",
    "            # Beta-annealing\n",
    "            beta = min(0.01 + ((epoch - 1) * (len(MyDataLoader)/args.batch_size) + index + 1) / args.epochs , 1)\n",
    "\n",
    "            if args.cuda:\n",
    "                aLS = aLS.cuda()\n",
    "                aLF = aLF.cuda()\n",
    "                aBD = aBD.cuda()\n",
    "                obs = obs.cuda()\n",
    "            aLS = aLS.view(-1,args.batch_size)\n",
    "            aLF = aLF.view(-1,args.batch_size)\n",
    "            aBD = aBD.view(-1,args.batch_size)\n",
    "            obs = obs.view(-1,args.batch_size)\n",
    "\n",
    "\n",
    "            #Pass throught the Graphical VI + flows  \n",
    "            q_LS_mu, q_LS_var, q_LS_0, q_LS_K, log_det_j_LS, q_LF_mu, q_LF_var, q_LF_0, q_LF_K, log_det_j_LF, q_BD_mu, q_BD_var, q_BD_0, q_BD_K, log_det_j_BD, log_p_xz, logp_zk = model(aLS, aLF, aBD, obs)\n",
    "\n",
    "            print('logp_zk: ', logp_zk)\n",
    "            print('log_p_xz: ', torch.mean(log_p_xz))\n",
    "            new_q_LS_K = q_LS_K.view(-1,1)\n",
    "            new_q_LF_K = q_LF_K.view(-1,1)\n",
    "            new_q_BD_K = q_BD_K.view(-1,1)\n",
    "            test_PLS[IND[i]] = new_q_LS_K\n",
    "            test_PLF[IND[i]] = new_q_LF_K\n",
    "            test_PBD[IND[i]] = new_q_BD_K\n",
    "\n",
    "\n",
    "            log_q_z0_LS = log_normal_dist(q_LS_0, mean=q_LS_mu, logvar=q_LS_var.log(), dim=1)\n",
    "            log_q_z0_LF = log_normal_dist(q_LF_0, mean=q_LF_mu, logvar=q_LF_var.log(), dim=1)\n",
    "            log_q_z0_BD = log_normal_dist(q_BD_0, mean=q_BD_mu, logvar=q_BD_var.log(), dim=1)\n",
    "\n",
    "            log_q_z0 = (log_q_z0_LS + log_q_z0_LF + log_q_z0_BD)/args.batch_size\n",
    "            print('log_q_z0: ',log_q_z0)\n",
    "\n",
    "            log_det_jacobians = log_det_j_LS + log_det_j_LF + log_det_j_BD\n",
    "            triplet_loss = nn.TripletMarginLoss(margin=1.0, p=2)\n",
    "            penalty_LS = triplet_loss(q_LS_K, aLS, aLF)\n",
    "            penalty_LF = triplet_loss(q_LF_K, aLF, aLS)\n",
    "\n",
    "\n",
    "            if args.anneal == \"std\":\n",
    "                #Equation (20)\n",
    "                kl = log_q_z0 - beta * logp_zk - log_det_jacobians #sum over batches\n",
    "                loss = beta * torch.abs(torch.mean(log_p_xz)) + kl + penalty_LS + penalty_LF\n",
    "            elif args.anneal == \"off\":\n",
    "                kl = torch.mean(log_q_z0 - logp_zk - log_det_jacobians) #sum over batches\n",
    "                loss = - torch.mean(log_p_xz) + kl + penalty_LS + penalty_LF\n",
    "            elif args.anneal == \"kl\":\n",
    "                kl = torch.mean(log_q_z0 - logp_zk - log_det_jacobians) #sum over batches\n",
    "                loss = - torch.mean(log_p_xz) + beta * kl + penalty_LS + penalty_LF\n",
    "        #     print('loss: ', loss)\n",
    "            # loss: log_q_0 - log_p_xz - log_p_zk - ldj\n",
    "\n",
    "\n",
    "        #Optimization step, Backpropagation\n",
    "            opt.zero_grad()\n",
    "            loss.backward(retain_graph=True)\n",
    "    #         torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
    "    #         for p in model.parameters():\n",
    "    #             p.data.add_(p.grad, alpha=-0.01)\n",
    "\n",
    "            train_loss[index] = loss.item()\n",
    "            opt.step()\n",
    "            t_loss = loss.item()\n",
    "\n",
    "\n",
    "            if index % args.log_interval == 0:\n",
    "                print('Epoch: {:3d} [({:2.0f}%)]  \\tLoss: {:11.6f}\\tkl: {:11.6f}'.format(\n",
    "                        epoch, 100. * index / len(MyDataLoader),\n",
    "                        loss.item(), kl.item()))\n",
    "            i += 1\n",
    "\n",
    "    #     print('====> Epoch: {:3d} Average train loss: {:.4f}'.format(\n",
    "    #             epoch, train_loss.sum() / len(MyDataLoader)))\n",
    "\n",
    "        TLoss.append(train_loss.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
